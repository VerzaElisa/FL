{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.initializers import GlorotUniform\n",
    "from keras.initializers import HeUniform\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.metrics import Precision, Recall\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('-'*25)\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import del dataset e divisione in train e test\n",
    "train_df = pd.read_csv('datasets/SpaceshipTitanic.csv')\n",
    "\n",
    "# Viene diviso il train set in train e validation set\n",
    "train, test_df = train_test_split(train_df, test_size = TEST_SIZE, random_state = 42)\n",
    "train_df, val_df = train_test_split(train, test_size = TEST_SIZE, random_state = 42)\n",
    "\n",
    "train_x = train_df.iloc[:,:-1]\n",
    "train_y = train_df.iloc[:,-1].astype(float)\n",
    "\n",
    "val_x = val_df.iloc[:,:-1]\n",
    "val_y = val_df.iloc[:,-1].astype(float)\n",
    "\n",
    "test_x = test_df.iloc[:,:-1]\n",
    "test_y = test_df.iloc[:,-1].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per calcolare la dipendenza tra feature categoriche con il chi-square test.\n",
    "# Il test ha come ipotesi nulla che le due variabili siano indipendenti. Quindi con un p-value\n",
    "# minore di 0.05 si rigetta l'ipotesi nulla e si accetta che le due variabili sono dipendenti.\n",
    "def chi_square(f1, f2):\n",
    "    crosstab_result=pd.crosstab(index=f1,columns=f2)\n",
    "    return chi2_contingency(crosstab_result)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home_planet\n",
    "# Si nota la forte correlazione tra group ed home planet/destination e quindi per i nan si possono assegnre i valori del gruppo. \n",
    "# Restano problematici i gruppi composti da una sola persona, si introduce quindi una nuova categoria di home planet e destination \n",
    "# per persona con origine o destinazione sconosciuta.\n",
    "\n",
    "def mv_hp_d(dataset):\n",
    "    print(\"-------------------------- HomePlanet -------------------------\")\n",
    "\n",
    "    # HomePlanet\n",
    "    p = chi_square(dataset['HomePlanet'], dataset['Group'])\n",
    "    print(f\"Le feature HomePlanet e Group sono correlate, con p-value{p}\" if p < 0.05 \\\n",
    "                                                                           else \"Le feature Destination e Group non sono correlate\")\n",
    "\n",
    "    print(f\"Numero di persone con HomePlanet nullo: {str(dataset['HomePlanet'].isna().sum())}\")\n",
    "    dataset['HomePlanet'][(dataset['Solo']==0)] = dataset[(dataset['Solo']==0)].groupby('Group')['HomePlanet'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "    dataset['HomePlanet'][(dataset['Solo']==1)] = dataset[(dataset['Solo']==1)].groupby('Group')['HomePlanet'].transform(lambda x: x.fillna(\"unknown\"))\n",
    "    print(f\"Numero di persone con HomePlanet nullo: {str(dataset['HomePlanet'].isna().sum())}\")\n",
    "\n",
    "    # Destination\n",
    "    print(\"-------------------------- Destination ------------------------\")\n",
    "    p = chi_square(dataset['Destination'], dataset['Group'])\n",
    "    print(f\"Le feature Destination e Group sono correlate, con p-value{p}\" if p < 0.05 \\\n",
    "                                                                           else \"Le feature Destination e Group non sono correlate\")\n",
    "    \n",
    "    print(f\"Numero di persone con Destination nullo: {str(dataset['Destination'].isna().sum())}\")\n",
    "    dataset['Destination'][(dataset['Solo']==0)] = dataset[(dataset['Solo']==0)].groupby('Group')['Destination'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "    dataset['Destination'][(dataset['Solo']==1)] = dataset[(dataset['Solo']==1)].groupby('Group')['Destination'].transform(lambda x: x.fillna(\"unknown\"))\n",
    "    print(f\"Numero di persone con Destination nullo: {str(dataset['Destination'].isna().sum())}\")\n",
    "\n",
    "# Age\n",
    "# Si distinguono 3 casi di persone con Age nullo: persone sole, persone con famiglia che non spendono e persone con famiglia che spendono.\n",
    "# Per assegnare l'età si usa la mediana del caso di appartenenza, immaginando che è meno probabile che una persona sola sia un bambino\n",
    "# e che al contrario è più probabile che una persona che non spende, ma è in un gruppo lo sia.\n",
    "def mv_age(dataset):\n",
    "    print(\"----------------------------- Age -----------------------------\")\n",
    "    solo = dataset['Solo']==1\n",
    "    family_no_spending = (dataset['Family_size']>1) & (dataset['No_spending']==1)\n",
    "    family_spending = (dataset['Family_size']>1) & (dataset['No_spending']==0)\n",
    "    \n",
    "    print(f\"Numero di persone con Age nullo: {str(dataset['Age'].isna().sum())}, di cui:\")\n",
    "    print(f\"Persone sole: {str(dataset['Age'][solo].isna().sum())}\")\n",
    "    print(f\"Persone con famiglia che non spendono: {str(dataset['Age'][family_no_spending].isna().sum())}\")   \n",
    "    print(f\"Persone con famiglia che spendono: {str(dataset['Age'][family_spending].isna().sum())}\")\n",
    "\n",
    "    dataset['Age'][solo] = dataset[solo]['Age'].transform(lambda x: x.fillna(dataset[solo]['Age'].median()))\n",
    "    dataset['Age'][family_no_spending] = dataset[family_no_spending]['Age'].transform(lambda x: x.fillna(dataset[family_no_spending]['Age'].median()))\n",
    "    dataset['Age'][family_spending] = dataset[family_spending]['Age'].transform(lambda x: x.fillna(dataset[family_spending]['Age'].median()))\n",
    "\n",
    "    print(f\"Numero di persone con Age nullo: {str(dataset['Age'].isna().sum())}\")   \n",
    "\n",
    "\n",
    "# Surname\n",
    "# Si osserva che i gruppi sono per gran parte composti da persone con lo stesso cognome, quindi si può assegnare la moda del cognome del gruppo\n",
    "# alle persone con cognome nullo. Per i gruppi composti da una sola persona si assegna cognome 'unknown'.\n",
    "def mv_surname(dataset):\n",
    "    print(\"------------------------------ Surname ------------------------\")\n",
    "\n",
    "    p = chi_square(dataset['Surname'], dataset['Group'])\n",
    "    print(f\"Le feature Surname e Group sono correlate, con p-value{p}\" if p < 0.05 \\\n",
    "                                                                       else \"Le feature Surname e Group non sono correlate\")\n",
    "\n",
    "    print(f\"Numero di persone con Surname nullo: {str(dataset['Surname'].isna().sum())}\")\n",
    "    dataset['Surname'][(dataset['Solo']==0)] = dataset[(dataset['Solo']==0)].groupby('Group')['Surname'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "    dataset['Surname'][(dataset['Solo']==1)] = dataset[(dataset['Solo']==1)].groupby('Group')['Surname'].transform(lambda x: x.fillna(\"Unknown\"))\n",
    "    print(f\"Numero di persone con Surname nullo: {str(dataset['Surname'].isna().sum())}\")\n",
    "\n",
    "# RoomService, FoodCourt, ShoppingMall, Spa, VRDeck\n",
    "# Si sostituiscono i valori nulli con la media di spesa delle altre categorie non nulle, dopo aver osservato che ogni persona ha almeno un valore\n",
    "# di spesa non nullo. Ad esempio, se una persona ha spesa nulla per RoomService e ShoppingMall, ma ha spesa per FoodCourt e Spa, si assegna la media\n",
    "# di FoodCourt e Spa per RoomService e ShoppingMall.\n",
    "def mv_exp(dataset):\n",
    "    print(\"------ RoomService, FoodCourt, ShoppingMall, Spa, VRDeck ------\")\n",
    "    print(f\"Persone con servizi nulli: {str(dataset[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].isnull().any(axis=1).sum())}\")\n",
    "    # Per ogni persona con almeno un servizio nullo, si calcola la media delle spese non nulle\n",
    "    dataset['RoomService'] = dataset['RoomService'].fillna(dataset[['FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].mean(axis=1))\n",
    "    dataset['FoodCourt'] = dataset['FoodCourt'].fillna(dataset[['RoomService', 'ShoppingMall', 'Spa', 'VRDeck']].mean(axis=1))\n",
    "    dataset['ShoppingMall'] = dataset['ShoppingMall'].fillna(dataset[['RoomService', 'FoodCourt', 'Spa', 'VRDeck']].mean(axis=1))\n",
    "    dataset['Spa'] = dataset['Spa'].fillna(dataset[['RoomService', 'FoodCourt', 'ShoppingMall', 'VRDeck']].mean(axis=1))\n",
    "    dataset['VRDeck'] = dataset['VRDeck'].fillna(dataset[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa']].mean(axis=1))\n",
    "    print(f\"Persone con servizi nulli: {str(dataset[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].isnull().any(axis=1).sum())}\")\n",
    "\n",
    "\n",
    "def mv_cabins(dataset):\n",
    "    print(\"------------------------- Cabin_side --------------------------\")\n",
    "    p = chi_square(dataset['Cabin_side'], dataset['Group'])\n",
    "    print(f\"Le feature Cabin_side e Group sono correlate, con p-value{p}\" if p < 0.05 \\\n",
    "                                                                          else \"Le feature Cabin_side e Group non sono correlate\")\n",
    "    print(f\"Numero di persone con Cabin_side nullo: {str(dataset['Cabin_side'].isna().sum())}\")\n",
    "    dataset['Cabin_side'][(dataset['Solo']==0)] = dataset[(dataset['Solo']==0)].groupby('Group')['Cabin_side'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "    dataset['Cabin_side'][(dataset['Solo']==1)] = dataset['Cabin_side'][(dataset['Solo']==1)].fillna('unknown')\n",
    "    print(f\"Numero di persone con Cabin_side nullo: {str(dataset['Cabin_side'].isna().sum())}\")\n",
    "\n",
    "    print(\"------------------------ Cabin_number -------------------------\")\n",
    "    p = chi_square(dataset['Cabin_number'], dataset['Group'])\n",
    "    print(f\"Le feature Cabin_number e Group sono correlate, con p-value{p}\" if p < 0.05 \\\n",
    "                                                                            else \"Le feature Cabin_number e Group non sono correlate\")\n",
    "    print(f\"Numero di persone con Cabin_number nullo: {str(dataset['Cabin_number'].isna().sum())}\")\n",
    "    dataset['Cabin_number'][(dataset['Solo']==0)] = dataset[(dataset['Solo']==0)].groupby('Group')['Cabin_number'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "    dataset['Cabin_number'][(dataset['Solo']==1)] = dataset['Cabin_number'][(dataset['Solo']==1)].fillna(0)\n",
    "    print(f\"Numero di persone con Cabin_number nullo: {str(dataset['Cabin_number'].isna().sum())}\")\n",
    "    \n",
    "    print(\"------------------------- Cabin_deck --------------------------\")\n",
    "    p = chi_square(dataset['Cabin_deck'], dataset['Group'])\n",
    "    print(f\"Le feature Cabin_deck e Group sono correlate, con p-value{p}\" if p < 0.05 \\\n",
    "                                                                               else \"Le feature Cabin_deck e Group non sono correlate\")\n",
    "    print(f\"Numero di persone con Cabin_deck nullo: {str(dataset['Cabin_deck'].isna().sum())}\")\n",
    "    dataset['Cabin_deck'][(dataset['Solo']==0)] = dataset[(dataset['Solo']==0)].groupby('Group')['Cabin_deck'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "    dataset['Cabin_deck'][(dataset['Solo']==1)] = dataset['Cabin_deck'][(dataset['Solo']==1)].fillna('unknown')\n",
    "    print(f\"Numero di persone con Cabin_deck nullo: {str(dataset['Cabin_deck'].isna().sum())}\")\n",
    "\n",
    "# VIP, CrioSleep\n",
    "# Per le feature VIP e CrioSleep si assegna il valore della moda a tutti i valori nulli.\n",
    "def mv_vip_crio(dataset):\n",
    "    print(\"----------------------------- VIP, CryoSleep ------------------\")\n",
    "    print(f\"Numero di persone con VIP nullo: {str(dataset['VIP'].isna().sum())}\")\n",
    "    dataset['VIP'] = dataset['VIP'].fillna(dataset['VIP'].mode()[0])\n",
    "    print(f\"Numero di persone con VIP nullo: {str(dataset['VIP'].isna().sum())}\")\n",
    "\n",
    "    print(f\"Numero di persone con CryoSleep nullo: {str(dataset['CryoSleep'].isna().sum())}\")\n",
    "    dataset['CryoSleep'] = dataset['CryoSleep'].fillna(dataset['CryoSleep'].mode()[0])\n",
    "    print(f\"Numero di persone con CryoSleep nullo: {str(dataset['CryoSleep'].isna().sum())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, train=True):\n",
    "    # Viene aggiunta una colonna per contare il numero di persone nel gruppo di appartenenza e se la persona è da sola\n",
    "    dataset['Group'] = dataset['PassengerId'].apply(lambda x: x.split('_')[0]).astype(int)\n",
    "    dataset['Group_size']=dataset['Group'].map(lambda x: dataset['Group'].value_counts()[x])\n",
    "    dataset['Solo']=(dataset['Group_size']==1).astype(int)\n",
    "    dataset.drop('PassengerId', axis=1, inplace=True)\n",
    "\n",
    "    # Viene creata una colonna per il cognome e si rimuove la colonna Name\n",
    "    dataset['Name'].fillna('Unknown Unknown', inplace=True)\n",
    "    dataset['Surname']=dataset['Name'].str.split().str[-1]\n",
    "    dataset.loc[dataset['Surname']=='Unknown','Surname']=np.nan\n",
    "    dataset.drop('Name', axis=1, inplace=True)\n",
    "\n",
    "    if train:\n",
    "        # Vengono riempiti i valori nulli delle colonne RoomService, FoodCourt, ShoppingMall, Spa, VRDeck\n",
    "        mv_exp(dataset)\n",
    "        # Vengono riempiti i valori nulli della colonna HomePlanet, Destination, Surmane\n",
    "        mv_hp_d(dataset)\n",
    "        mv_surname(dataset)\n",
    "    \n",
    "    # Vengono prese tutte le colonne che rappresentano le spese e si aggiunge una colonna per contare il\n",
    "    # totale speso dalla persona e una colonna booleana per indicare se la persona non ha speso nulla\n",
    "    exp_feats=['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "    dataset['Expenditure']=dataset[exp_feats].sum(axis=1)\n",
    "    dataset['No_spending']=(dataset['Expenditure']==0).astype(int)\n",
    "\n",
    "    # Viene aggiunta una colonna per contare le persone che hanno lo stesso cognome\n",
    "    dataset['Family_size']=dataset[(dataset['Surname']!='Unknown')].groupby('Group')['Surname'].transform('count')\n",
    "    dataset.loc[dataset['Surname']=='Unknown','Family_size']=1\n",
    "    dataset.loc[dataset['Family_size']>100,'Family_size']=np.nan\n",
    "    dataset.drop('Surname', axis=1, inplace=True)\n",
    "\n",
    "    # Separazione della colonna 'Cabin' nelle colonne deck, number e side\n",
    "    dataset['Cabin'].fillna('Z/9999/Z', inplace=True)\n",
    "\n",
    "    dataset['Cabin_deck'] = dataset['Cabin'].apply(lambda x: x.split('/')[0])\n",
    "    dataset['Cabin_number'] = dataset['Cabin'].apply(lambda x: x.split('/')[1]).astype(int)\n",
    "    dataset['Cabin_side'] = dataset['Cabin'].apply(lambda x: x.split('/')[2])\n",
    "\n",
    "    dataset.loc[dataset['Cabin_deck']=='Z', 'Cabin_deck']=np.nan\n",
    "    dataset.loc[dataset['Cabin_number']==9999, 'Cabin_number']=np.nan\n",
    "    dataset.loc[dataset['Cabin_side']=='Z', 'Cabin_side']=np.nan\n",
    "\n",
    "    dataset.drop('Cabin', axis=1, inplace=True)\n",
    "    if train:\n",
    "        # Vengono riempiti i valori nulli della colonna Age\n",
    "        mv_age(dataset)\n",
    "        mv_cabins(dataset)\n",
    "        mv_vip_crio(dataset)\n",
    "        dataset['VIP'].astype(object)\n",
    "        dataset['CryoSleep'].astype(object)\n",
    "\n",
    "    # Viene aggiunta una colonna che assegna ogni passeggero al gruppo di età di appartenenza\n",
    "    dataset['Age_group']=0\n",
    "    dataset.loc[dataset['Age']<=12,'Age_group']='Age_0-12'\n",
    "    dataset.loc[(dataset['Age']>12) & (dataset['Age']<18),'Age_group']='Age_13-17'\n",
    "    dataset.loc[(dataset['Age']>=18) & (dataset['Age']<=25),'Age_group']='Age_18-25'\n",
    "    dataset.loc[(dataset['Age']>25) & (dataset['Age']<=30),'Age_group']='Age_26-30'\n",
    "    dataset.loc[(dataset['Age']>30) & (dataset['Age']<=50),'Age_group']='Age_31-50'\n",
    "    dataset.loc[dataset['Age']>50,'Age_group']='Age_51+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_set = train_x.copy()\n",
    "preprocess_data(hm_set, False)\n",
    "\n",
    "hm_set = hm_set.apply(lambda col: col.astype(str) if col.dtype == 'object' else col)\n",
    "num_cols = hm_set.select_dtypes(exclude='object').columns\n",
    "cat_cols = hm_set.select_dtypes(include='object').columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "hm_set[num_cols] = scaler.fit_transform(hm_set[num_cols])\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "hm_set[cat_cols] = encoder.fit_transform(hm_set[cat_cols])\n",
    "\n",
    "correlation_matrix=hm_set.corr()\n",
    "correlation_matrix\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(dataset, ord_list):\n",
    "    preprocess_data(dataset)\n",
    "\n",
    "    fill_dict = {col: dataset[col].mean() if dataset[col].dtype != 'object' else dataset[col].mode()[0] for col in dataset.columns}\n",
    "\n",
    "    num_cols = dataset.select_dtypes(exclude=['object', 'bool']).columns.difference(ord_list)\n",
    "    cat_cols = dataset.select_dtypes(include=['object', 'bool']).columns.difference(ord_list)\n",
    "    print(train_x.dtypes)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    dataset[num_cols] = scaler.fit_transform(dataset[num_cols])\n",
    "\n",
    "    encoder = OrdinalEncoder()\n",
    "    dataset[ord_list] = encoder.fit_transform(dataset[ord_list])\n",
    "\n",
    "    dataset = pd.get_dummies(dataset, columns=cat_cols)\n",
    "\n",
    "    return dataset.astype(float), fill_dict, scaler, encoder\n",
    "\n",
    "\n",
    "ord_list = ['Cabin_number']\n",
    "train_x, mean, scaler, encoder = encoding(train_x, ord_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "str_path = 'objects/features_titanic.npy'\n",
    "def fs(dataset, dataset_y):\n",
    "\n",
    "    if not os.path.exists(str_path):\n",
    "        dataset['Survived'] = dataset_y\n",
    "        correlation_matrix=dataset.corr()\n",
    "        features = correlation_matrix['Survived'][(correlation_matrix['Survived']>=0.1) | (correlation_matrix['Survived']<=-0.1)].index\n",
    "        features = features.drop('Survived')\n",
    "        np.save(str_path, features)\n",
    "    features = np.load(str_path, allow_pickle=True)\n",
    "    return train_x[features]\n",
    "train_x = fs(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_val_test(dataset, mean, scaler, ord_list):  \n",
    "\n",
    "    preprocess_data(dataset, False)\n",
    "    print(dataset[ord_list])\n",
    "\n",
    "    for key in mean.keys():\n",
    "        dataset[key].fillna(mean[key], inplace=True)\n",
    "\n",
    "    cat_cols = dataset.select_dtypes(include=['object', 'bool']).columns.difference(ord_list)\n",
    "    dataset = pd.get_dummies(dataset, columns=cat_cols)\n",
    "\n",
    "    num_cols = dataset.select_dtypes(exclude=['object', 'bool']).columns.difference(ord_list)\n",
    "\n",
    "    dataset[num_cols] = scaler.transform(dataset[num_cols])\n",
    "\n",
    "    features = np.load('objects/features_titanic.npy', allow_pickle=True)\n",
    "    dataset = dataset[features]\n",
    "    dataset = dataset.astype(float)\n",
    "    print(val_x.isnull().sum())\n",
    "    return dataset\n",
    "val_x = preproc_val_test(val_x, mean, scaler, ord_list)\n",
    "test_x = preproc_val_test(test_x, mean, scaler, ord_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 512\n",
    "DROPOUT = 0.1\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def model_fn():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "\n",
    "    model.add(Dense(train_x.shape[-1], kernel_initializer = HeUniform(), activation = 'relu', input_dim = train_x.shape[-1]))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(1024, kernel_initializer = HeUniform(), activation = 'relu', kernel_regularizer = tf.keras.regularizers.l2(30e-6)))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(256, kernel_initializer = HeUniform(), activation = 'relu', kernel_regularizer = tf.keras.regularizers.l2(30e-6)))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(128, kernel_initializer = HeUniform(), activation = 'relu', kernel_regularizer = tf.keras.regularizers.l2(30e-6)))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(1, kernel_initializer = GlorotUniform(), activation = 'sigmoid'))\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = model_fn()\n",
    "\n",
    "initial_learning_rate = 0.01\n",
    "final_learning_rate = 0.0001\n",
    "learning_rate_decay_factor = (final_learning_rate / initial_learning_rate)**(1/EPOCHS)\n",
    "steps_per_epoch = int(train_x.shape[0]/BATCH_SIZE)\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=steps_per_epoch,\n",
    "    decay_rate=learning_rate_decay_factor,\n",
    "    staircase=True)\n",
    "model.compile(optimizer = keras.optimizers.Adam(learning_rate=lr_schedule), loss = 'binary_focal_crossentropy', metrics = ['accuracy', Precision(), Recall()])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20, min_delta=0.0001, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_x, train_y,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=[early_stopping_cb],\n",
    "                    validation_data=(val_x, val_y))\n",
    "\n",
    "EPOCHS = len(history.history['loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "axs[0, 0].plot(history.history['val_accuracy'])\n",
    "axs[0, 0].plot(history.history['accuracy'])\n",
    "axs[0, 0].legend(['val_accuracy', 'accuracy'])\n",
    "\n",
    "axs[0, 1].plot(history.history['val_precision'])\n",
    "axs[0, 1].plot(history.history['precision'])\n",
    "axs[0, 1].legend(['val_precision', 'precision'])\n",
    "\n",
    "axs[1, 0].plot(history.history['val_recall'])\n",
    "axs[1, 0].plot(history.history['recall'])\n",
    "axs[1, 0].legend(['val_recall', 'recall'])\n",
    "\n",
    "axs[1, 1].plot(history.history['val_loss'])\n",
    "axs[1, 1].plot(history.history['loss'])\n",
    "axs[1, 1].legend(['val_loss', 'loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_x, test_y)\n",
    "print(score[0])\n",
    "print(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train.iloc[:,:-1]\n",
    "train_y = train.iloc[:,-1].astype(float)\n",
    "\n",
    "test_x = test_df.iloc[:,:-1]\n",
    "test_y = test_df.iloc[:,-1].astype(float)\n",
    "\n",
    "ord_list = ['Cabin_number']\n",
    "train_x, mean, scaler, encoder = encoding(train_x, ord_list)\n",
    "train_x = fs(train_x, train_y)\n",
    "\n",
    "test_x = preproc_val_test(test_x, mean, scaler, ord_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_fn() \n",
    "model.compile(optimizer = keras.optimizers.Adam(learning_rate=lr_schedule), loss = 'binary_focal_crossentropy', metrics = ['accuracy', Precision(), Recall()])\n",
    "\n",
    "history = model.fit(train_x, train_y,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE)\n",
    "\n",
    "score = model.evaluate(test_x, test_y)\n",
    "\n",
    "\n",
    "train_df = pd.concat([train_x, train_y], axis=1)\n",
    "train_df.to_csv('datasets/train_titanic.csv', index=False)\n",
    "test_df = pd.concat([test_x, test_y], axis=1)\n",
    "test_df.to_csv('datasets/test_titanic.csv', index=False)\n",
    "\n",
    "model.save('models/titanic_model')\n",
    "model.save_weights('models/titanic_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict(test_x)\n",
    "y_pred = (y_pred > 0.5)\n",
    "cm = confusion_matrix(test_y, y_pred)\n",
    "\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(5, 2))\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
