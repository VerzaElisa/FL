{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BKyHkMxKHfV"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.initializers import GlorotUniform\n",
        "\n",
        "\n",
        "TEST_SIZE = 0.2\n",
        "NUM_CLIENTS = 4\n",
        "BATCH_SIZE = 512\n",
        "DROPOUT = 0.2\n",
        "EPOCHS = 10\n",
        "PREFETCH_BUFFER = 10\n",
        "NUM_ROUNDS = 10\n",
        "UNBALANCED = False\n",
        "path = os.path.dirname(tff.__file__)\n",
        "print(path)\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "NayDhCX6SjwE"
      },
      "outputs": [],
      "source": [
        "# Import del dataset e divisione in train e test\n",
        "train_df = pd.read_csv('datasets/train_internet.csv')\n",
        "test_df = pd.read_csv('datasets/test_internet.csv')\n",
        "\n",
        "train_x = train_df.drop(columns=['label'])\n",
        "train_y = train_df['label'].astype(int)\n",
        "\n",
        "test_x = test_df.drop(columns=['label'])\n",
        "test_y = test_df['label'].astype(int)\n",
        "\n",
        "# Funzione per il preprocessing dei dati del singolo client che divide il dataset in batch\n",
        "def preprocess(dataset):\n",
        "  return dataset.repeat(EPOCHS).batch(BATCH_SIZE).prefetch(PREFETCH_BUFFER)\n",
        "\n",
        "# Funzione per aggiungere una colonna client_num al dataset in modo tale che ogni client abbia una percentuale di \n",
        "# righe del dataset diversa.\n",
        "def client_unbalanced(dataset, num_clients):\n",
        "    client_num = []\n",
        "    prob = np.random.pareto(1, num_clients)\n",
        "    prob /= np.sum(prob)\n",
        "    print(f'Le probabilit√† per client sono {prob}')\n",
        "    for i in range(len(dataset)):\n",
        "        client_num.append(np.random.choice(num_clients, p=prob))\n",
        "    dataset['client_num'] = client_num\n",
        "    return dataset\n",
        "\n",
        "# Funzione per aggiungere una colonna client_num al dataset in modo tale che ogni client peschi esempi da un numero di classi che sia \n",
        "# il totale meno un numero passato come parametro\n",
        "def distribute_clients(dataset, exclude_count):\n",
        "    num_clients = 4\n",
        "    client_0 = []\n",
        "    client_1 = []\n",
        "    client_2 = []\n",
        "    client_3 = []\n",
        "    dl = dataset['label'].unique()\n",
        "  \n",
        "    for client in range(num_clients):\n",
        "        classes_to_exclude = []\n",
        "        for i in range(exclude_count):\n",
        "            if client+i < len(dl):\n",
        "                inc = dl[client+i]\n",
        "                classes_to_exclude.append(inc)\n",
        "            else:\n",
        "                classes_to_exclude.append(dl[client+i-len(dl)])\n",
        "        classes_to_include = [x for x in dl if x not in classes_to_exclude]\n",
        "        print(classes_to_include)\n",
        "        for class_label in classes_to_include:\n",
        "            class_label = int(class_label)\n",
        "            class_indices = dataset.index[dataset['label'] == class_label].tolist()\n",
        "            if(class_label == 0):\n",
        "                client_0.extend([client] * (math.ceil(len(class_indices)/(4-exclude_count))))\n",
        "            elif(class_label == 1):\n",
        "                client_1.extend([client] * (math.ceil(len(class_indices)/(4-exclude_count))))   \n",
        "            elif(class_label == 2):\n",
        "                client_2.extend([client] * (math.ceil(len(class_indices)/(4-exclude_count))))\n",
        "            else:\n",
        "                client_3.extend([client] * (math.ceil(len(class_indices)/(4-exclude_count))))\n",
        "    client_ids = client_0 + client_1 + client_2 + client_3\n",
        "    if len(client_ids) > len(dataset):\n",
        "        client_ids = client_ids[:len(dataset)]\n",
        "        \n",
        "    dataset = dataset.sort_values(by='label')\n",
        "    dataset['client_num'] = client_ids\n",
        "    return dataset\n",
        "\n",
        "# Funzione per la creazione di un dataset ClientData a partire dal dataset di training a cui viene\n",
        "# aggiunta una colonna client_num che assegna ad ogni riga un client randomico\n",
        "def create_clients(dataset, unbalanced, num_clients=NUM_CLIENTS, exclude_count=0):\n",
        "    if unbalanced==1: \n",
        "        dataset = client_unbalanced(dataset, num_clients)\n",
        "    elif unbalanced==0:\n",
        "        # Viene creata una lista di client\n",
        "        clients = np.random.uniform(0, 1, len(dataset))\n",
        "        dataset['client_num'] = clients\n",
        "    elif unbalanced==2:\n",
        "        dataset = distribute_clients(dataset, exclude_count)\n",
        "    else:\n",
        "        ret = 'non valido'\n",
        "        return ret\n",
        "\n",
        "    # Viene convertito il dataset in dizionari, uno per ogni client, con label e pixel associati\n",
        "    client_train_dataset = collections.OrderedDict()\n",
        "    grouped_dataset = dataset.groupby('client_num')\n",
        "    for key, item in grouped_dataset:\n",
        "        current_client = grouped_dataset.get_group(key)\n",
        "        data = collections.OrderedDict((('y', train_y), ('x', train_x)))\n",
        "        client_train_dataset[key] = data\n",
        "\n",
        "    # I dizionari vengono convertiti in ClientDataset\n",
        "    def serializable_dataset_fn(client_id):\n",
        "        client_data = client_train_dataset[client_id]\n",
        "        return tf.data.Dataset.from_tensor_slices(client_data)\n",
        "\n",
        "    tff_train_data = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
        "        client_ids=list(client_train_dataset.keys()),\n",
        "        serializable_dataset_fn=serializable_dataset_fn\n",
        "    )\n",
        "\n",
        "    return tff_train_data\n",
        "\n",
        "# Creazione della lista contenente i client con i relativi dataset\n",
        "elem_spec = {}\n",
        "def init(dataset, active_clients=NUM_CLIENTS, unbalanced=0, exclude_count=0): \n",
        "    client_data_df = create_clients(dataset, unbalanced, active_clients, exclude_count)\n",
        "    client_ids = sorted(client_data_df.client_ids)[:active_clients]\n",
        "    return [preprocess(client_data_df.create_tf_dataset_for_client(x)) for x in client_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "LYCsJGJFWbqt"
      },
      "outputs": [],
      "source": [
        "def create_keras_model():\n",
        "  model = Sequential()\n",
        "  seed = 1\n",
        "\n",
        "  model.add(Dense(112, activation='softmax', kernel_initializer=GlorotUniform(seed), input_dim=train_x.shape[-1]))\n",
        "  model.add(Dropout(DROPOUT))\n",
        "  model.add(Dense(272, kernel_initializer=GlorotUniform(seed),activation='tanh'))\n",
        "  model.add(Dropout(DROPOUT))\n",
        "  model.add(Dense(4, kernel_initializer=GlorotUniform(seed), activation='softmax'))   \n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Q3ynrxd53HzY"
      },
      "outputs": [],
      "source": [
        "def model_fn():\n",
        "  keras_model = create_keras_model()\n",
        "  return tff.learning.models.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=elem_spec,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "def aggregator(algo, prox):\n",
        "    if algo == 'weighted avg':\n",
        "        training_process = tff.learning.algorithms.build_weighted_fed_avg(model_fn, \n",
        "                                                                          client_optimizer_fn=tff.learning.optimizers.build_adam(learning_rate=0.01),\n",
        "                                                                          server_optimizer_fn=tff.learning.optimizers.build_adam(learning_rate=0.01))\n",
        "\n",
        "    if algo == 'unweighted avg':\n",
        "        training_process = tff.learning.algorithms.build_unweighted_fed_avg(model_fn, \n",
        "                                                                            client_optimizer_fn=tff.learning.optimizers.build_adam(learning_rate=0.01),\n",
        "                                                                            server_optimizer_fn=tff.learning.optimizers.build_adam(learning_rate=0.01))\n",
        "\n",
        "    if algo == 'weighted prox':\n",
        "        training_process = tff.learning.algorithms.build_weighted_fed_prox(model_fn, \n",
        "                                                                           proximal_strength=prox, \n",
        "                                                                           client_optimizer_fn=tff.learning.optimizers.build_adam(learning_rate=0.01),\n",
        "                                                                           server_optimizer_fn=tff.learning.optimizers.build_adam(learning_rate=0.01))\n",
        "    if algo == 'unweighted prox':\n",
        "        training_process = tff.learning.algorithms.build_weighted_fed_prox(model_fn, \n",
        "                                                                           proximal_strength=prox,\n",
        "                                                                           client_optimizer_fn=tff.learning.optimizers.build_adam(learning_rate=0.01),\n",
        "                                                                           server_optimizer_fn=tff.learning.optimizers.build_adam(learning_rate=0.01))\n",
        "    return training_process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrJkQuCRJP9C"
      },
      "outputs": [],
      "source": [
        "federated_train_data = init(train_df)\n",
        "elem_spec = federated_train_data[0].element_spec\n",
        "training_process = aggregator('weighted avg', 20.0)\n",
        "train_state = training_process.initialize()\n",
        "for round_num in range(NUM_ROUNDS):\n",
        "  result = training_process.next(train_state, federated_train_data)\n",
        "  train_state = result.state\n",
        "  train_metrics = result.metrics\n",
        "  print('round {:2d}, metrics={}'.format(round_num, train_metrics))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "e5fGtIJYNqYH"
      },
      "outputs": [],
      "source": [
        "def keras_evaluate(state, training_process):\n",
        "  keras_model = create_keras_model()\n",
        "  keras_model.compile(\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "  model_weights = training_process.get_model_weights(state)\n",
        "  model_weights.assign_weights_to(keras_model)\n",
        "  loss, accuracy = keras_model.evaluate(x=test_x, y=test_y)\n",
        "  print('\\tEval: loss={l:.3f}, accuracy={a:.3f}'.format(l=loss, a=accuracy))\n",
        "  return loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "keras_evaluate(train_state, training_process)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esperimenti\n",
        "==============\n",
        "\n",
        "***Algoritmo di aggregazione***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "federated_train_data = init(train_df)\n",
        "elem_spec = federated_train_data[0].element_spec\n",
        "# Tuning del parametro di proximal strength\n",
        "def tune_proximal_strength():\n",
        "    prox_list = []\n",
        "    for i in [1.0, 10.0, 20.0, 128.0, 256.0, 512.0]:\n",
        "        training_process = aggregator('weighted prox', i)\n",
        "        train_state = training_process.initialize()\n",
        "        curr = []\n",
        "        for round_num in range(NUM_ROUNDS):\n",
        "            result = training_process.next(train_state, federated_train_data)\n",
        "            train_state = result.state\n",
        "            train_metrics = result.metrics\n",
        "            print('round {:2d}, metrics={}'.format(round_num, train_metrics))\n",
        "            acc_tuple = (round_num, \n",
        "                         train_metrics['client_work']['train']['sparse_categorical_accuracy'])\n",
        "            curr.append(acc_tuple)\n",
        "        prox_list.append((i, curr))\n",
        "\n",
        "    return prox_list\n",
        "\n",
        "prox_list = tune_proximal_strength()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i = 0\n",
        "rounds = []\n",
        "accuracies = []\n",
        "\n",
        "for algo, acc_list in prox_list:\n",
        "    rounds.append([x[0] for x in acc_list])\n",
        "    accuracies.append([x[1] for x in acc_list])\n",
        "    i+=1\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "for i in range(len(prox_list)):\n",
        "    ax.plot(rounds[i], accuracies[i], label='prox strength = {}'.format(prox_list[i][0]))\n",
        "ax.set(xlabel='rounds', ylabel='accuracy',\n",
        "       title='Proximal strength tuning')\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "federated_train_data = init(train_df, unbalanced=1)\n",
        "elem_spec = federated_train_data[0].element_spec\n",
        "\n",
        "def agg_experiment():\n",
        "    prox_list = []\n",
        "    for i in ['weighted avg', 'unweighted avg', 'weighted prox', 'unweighted prox']:\n",
        "        training_process = aggregator(i, 1.0)\n",
        "        train_state = training_process.initialize()\n",
        "        curr = []\n",
        "        for round_num in range(NUM_ROUNDS):\n",
        "            result = training_process.next(train_state, federated_train_data)\n",
        "            train_state = result.state\n",
        "            train_metrics = result.metrics\n",
        "            print('round {:2d}, metrics={}'.format(round_num, train_metrics))\n",
        "            acc_tuple = (round_num, \n",
        "                         train_metrics['client_work']['train']['sparse_categorical_accuracy'])\n",
        "            curr.append(acc_tuple)\n",
        "        prox_list.append((i, curr))\n",
        "    return prox_list\n",
        "\n",
        "agg_algo_list = agg_experiment()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i = 0\n",
        "rounds = []\n",
        "accuracies = []\n",
        "\n",
        "for algo, acc_list in agg_algo_list:\n",
        "    rounds.append([x[0] for x in acc_list])\n",
        "    accuracies.append([x[1] for x in acc_list])\n",
        "    i+=1\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "for i in range(len(agg_algo_list)):\n",
        "    ax.plot(rounds[i], accuracies[i], label='prox strength = {}'.format(agg_algo_list[i][0]))\n",
        "ax.set(xlabel='rounds', ylabel='accuracy',\n",
        "       title='Aggregation Algorithm')\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Numero e Percentuale clients***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "def client_perc_experiment():\n",
        "    client_list = []\n",
        "    eval_list = []\n",
        "    for i in [10, 50, 100]:\n",
        "        perc_list = []\n",
        "        for j in [0.25, 0.50, 0.75, 1]:\n",
        "            federated_train_data = init(train_df, active_clients=math.floor(i*j))\n",
        "            global elem_spec \n",
        "            elem_spec = federated_train_data[0].element_spec\n",
        "            training_process = aggregator('weighted avg', 1.0)\n",
        "            train_state = training_process.initialize()\n",
        "  \n",
        "            curr = []\n",
        "            for round_num in range(NUM_ROUNDS):\n",
        "                result = training_process.next(train_state, federated_train_data)\n",
        "                train_state = result.state\n",
        "                train_metrics = result.metrics\n",
        "                print('round {:2d}, metrics={}'.format(round_num, train_metrics))\n",
        "                acc_tuple = (round_num, \n",
        "                             train_metrics['client_work']['train']['sparse_categorical_accuracy'])\n",
        "                curr.append(acc_tuple)\n",
        "            eval = keras_evaluate(train_state, training_process)\n",
        "            eval_list.append((i, j, eval))\n",
        "            perc_list.append((j, curr))\n",
        "        client_list.append((i, perc_list))\n",
        "    return client_list, eval_list\n",
        "\n",
        "client_list, eval_list = client_perc_experiment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_metric(data, metric_index, metric_name):\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    for i, (clients, portions_data) in enumerate(data):\n",
        "        ax = axs[i // 2, i % 2]\n",
        "        for portion, epoch_data in portions_data:\n",
        "            epochs = [e[0] for e in epoch_data]\n",
        "            metric_values = [e[metric_index] for e in epoch_data]\n",
        "            ax.plot(epochs, metric_values, label=f'{portion*100}% data')\n",
        "        \n",
        "        ax.set_title(f'{clients} Clients')\n",
        "        ax.set_xlabel('Metrics')\n",
        "        ax.set_ylabel(metric_name)\n",
        "        ax.legend()\n",
        "        ax.grid(True) \n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "plot_metric(client_list, 1, 'Accuracy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(eval_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Distribuzione classi***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def client_perc_experiment():\n",
        "    eval_list = []\n",
        "    perc_list = []\n",
        "    for i in [0, 1, 2, 3]:\n",
        "        federated_train_data = init(train_df, active_clients=NUM_CLIENTS, unbalanced=2, exclude_count=i)\n",
        "        global elem_spec \n",
        "        elem_spec = federated_train_data[0].element_spec\n",
        "        training_process = aggregator('weighted avg', 1.0)\n",
        "        train_state = training_process.initialize()\n",
        "\n",
        "        curr = []\n",
        "        for round_num in range(NUM_ROUNDS):\n",
        "            result = training_process.next(train_state, federated_train_data)\n",
        "            train_state = result.state\n",
        "            train_metrics = result.metrics\n",
        "            print('round {:2d}, metrics={}'.format(round_num, train_metrics))\n",
        "            acc_tuple = (round_num, \n",
        "                         train_metrics['client_work']['train']['sparse_categorical_accuracy'])\n",
        "            curr.append(acc_tuple)\n",
        "        eval = keras_evaluate(train_state, training_process)\n",
        "        eval_list.append((i, eval))\n",
        "        perc_list.append((i, curr))\n",
        "    return perc_list, eval_list\n",
        "    \n",
        "perc_list, eval_list = client_perc_experiment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rounds = []\n",
        "accuracies = []\n",
        "\n",
        "for algo, acc_list in perc_list:\n",
        "    rounds.append([x[0] for x in acc_list])\n",
        "    accuracies.append([x[1] for x in acc_list])\n",
        "\n",
        "def plot_metrics(ax, rounds, metrics, labels=['4 classi','3 classi', '2 classi', '1 classe']):\n",
        "    for i in range(len(metrics)):\n",
        "        ax.plot(rounds[i], metrics[i], label=labels[i])\n",
        "        ax.legend(loc='lower right')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "plot_metrics(ax, rounds, accuracies)\n",
        "\n",
        "ax.set_title('Accuracy')\n",
        "ax.set_xlabel('Rounds')\n",
        "ax.set_ylabel('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "federated_learning_for_image_classification.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
