{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8BKyHkMxKHfV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/ella/Documents/FL/venv-federated/lib/python3.9/site-packages/tensorflow_federated\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_federated as tff\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense\n",
        "from keras.initializers import GlorotUniform\n",
        "from keras.initializers import HeUniform\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "\n",
        "TEST_SIZE = 0.2\n",
        "NUM_CLIENTS = 100\n",
        "ACTIVE_CLIENTS = 100\n",
        "BATCH_SIZE = 512\n",
        "DROPOUT = 0.1\n",
        "EPOCHS = 10\n",
        "PREFETCH_BUFFER = 10\n",
        "NUM_ROUNDS = 10\n",
        "path = os.path.dirname(tff.__file__)\n",
        "print(path)\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NayDhCX6SjwE"
      },
      "outputs": [],
      "source": [
        "# Import del dataset e divisione in train e test\n",
        "df = pd.read_csv('datasets/data.csv')\n",
        "# riempire i nan con 0\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size = TEST_SIZE, random_state = 42)\n",
        "\n",
        "# Funzione per il preprocessing dei dati del singolo client che divide il dataset in batch\n",
        "def preprocess(dataset):\n",
        "  return dataset.repeat(EPOCHS).batch(BATCH_SIZE).prefetch(PREFETCH_BUFFER)\n",
        "\n",
        "# Funzione per la creazione di un dataset ClientData a partire dal dataset di training a cui viene\n",
        "# aggiunta una colonna client_nums che assegna ad ogni riga un client randomico\n",
        "def create_clients(dataset):\n",
        "    # Viene creata una lista randomica di client\n",
        "    client_nums = list(range(NUM_CLIENTS))\n",
        "    generator = np.random.default_rng(42)\n",
        "    clients = generator.choice(client_nums, len(dataset))\n",
        "    dataset['client_nums'] = clients\n",
        "\n",
        "    # Viene convertito il dataset in dizionari, uno per ogni client, con label e pixel associati\n",
        "    client_train_dataset = collections.OrderedDict()\n",
        "    grouped_dataset = dataset.groupby('client_nums')\n",
        "    for key, item in grouped_dataset:\n",
        "        current_client = grouped_dataset.get_group(key)\n",
        "        data = collections.OrderedDict((('y',current_client.iloc[:,-2]), ('x', current_client.iloc[:,:-2])))\n",
        "        client_train_dataset[key] = data\n",
        "\n",
        "    # I dizionari vengono convertiti in ClientDataset\n",
        "    def serializable_dataset_fn(client_id):\n",
        "        client_data = client_train_dataset[client_id]\n",
        "        return tf.data.Dataset.from_tensor_slices(client_data)\n",
        "\n",
        "    tff_train_data = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
        "        client_ids=list(client_train_dataset.keys()),\n",
        "        serializable_dataset_fn=serializable_dataset_fn\n",
        "    )\n",
        "\n",
        "    return tff_train_data\n",
        "\n",
        "# Creazione della lista contenente i client con i relativi dataset\n",
        "client_data_df = create_clients(train_df)\n",
        "client_ids = sorted(client_data_df.client_ids)[:ACTIVE_CLIENTS]\n",
        "federated_train_data = [preprocess(client_data_df.create_tf_dataset_for_client(x)) for x in client_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('y',\n",
              "              array([1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                     0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
              "                     0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
              "                     1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
              "                     0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
              "                     1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
              "                     1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
              "                     1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
              "                     0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
              "                     1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
              "                     1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
              "                     0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
              "                     0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
              "                     0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
              "                     0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "                     1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
              "                     0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
              "                     1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
              "                     1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
              "                     0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
              "                     0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
              "                     0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
              "                     1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
              "                     1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
              "                     0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
              "                     1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
              "                     1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
              "                     0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
              "                     1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
              "                     0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
              "                     1., 1.])),\n",
              "             ('x',\n",
              "              array([[-0.72595401, -0.05224867, -0.15148573, ...,  0.        ,\n",
              "                       0.        ,  0.        ],\n",
              "                     [ 1.37749773, -0.05224867, -0.15148573, ...,  0.        ,\n",
              "                       0.        ,  0.        ],\n",
              "                     [ 1.37749773,  1.55435651, -0.15148573, ...,  0.        ,\n",
              "                       0.        ,  0.        ],\n",
              "                     ...,\n",
              "                     [-0.72595401,  0.99553731,  6.60128193, ...,  0.        ,\n",
              "                       1.        ,  0.        ],\n",
              "                     [ 1.37749773,  2.0433233 , -0.15148573, ...,  1.        ,\n",
              "                       0.        ,  0.        ],\n",
              "                     [-0.72595401, -0.96032986, -0.15148573, ...,  1.        ,\n",
              "                       0.        ,  0.        ]]))])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_batch = tf.nest.map_structure(lambda x: x.numpy(),\n",
        "                                     next(iter(federated_train_data[0])))\n",
        "sample_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LYCsJGJFWbqt"
      },
      "outputs": [],
      "source": [
        "def create_keras_model():\n",
        "  model = Sequential()\n",
        "  # layers\n",
        "  model.add(Dense(64, kernel_initializer = HeUniform(), activation = 'relu', input_dim = 42))\n",
        "  model.add(Dropout(DROPOUT))\n",
        "  model.add(Dense(10, kernel_initializer = HeUniform(), activation = 'relu'))\n",
        "  model.add(Dense(1, kernel_initializer = GlorotUniform(), activation = 'sigmoid'))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Q3ynrxd53HzY"
      },
      "outputs": [],
      "source": [
        "def model_fn():\n",
        "  # We _must_ create a new model here, and _not_ capture it from an external\n",
        "  # scope. TFF will call this within different graph contexts.\n",
        "  keras_model = create_keras_model()\n",
        "  return tff.learning.models.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=federated_train_data[0].element_spec,\n",
        "      loss=tf.keras.losses.BinaryFocalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.Accuracy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sk6mjOfycX5N"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-09-20 15:43:57.371086: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2024-09-20 15:43:57.371196: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
            "2024-09-20 15:43:57.400536: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2024-09-20 15:43:57.400645: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n"
          ]
        }
      ],
      "source": [
        "training_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=tff.learning.optimizers.build_adam(learning_rate=0.0001),\n",
        "    server_optimizer_fn=tff.learning.optimizers.build_adam(learning_rate=0.0001))\n",
        "train_state = training_process.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qrJkQuCRJP9C"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-09-20 15:43:57.886981: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2024-09-20 15:43:57.887076: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
            "2024-09-20 15:43:58.040092: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2024-09-20 15:43:58.040188: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
            "2024-09-20 15:43:58.047485: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2024-09-20 15:43:58.047589: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
            "2024-09-20 15:43:58.055665: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2024-09-20 15:43:58.055752: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
            "2024-09-20 15:43:58.068772: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2024-09-20 15:43:58.068868: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
            "2024-09-20 15:43:58.091131: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2024-09-20 15:43:58.091220: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
            "2024-09-20 15:43:58.100202: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2024-09-20 15:43:58.100307: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
            "2024-09-20 15:43:58.107881: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2024-09-20 15:43:58.107959: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
            "2024-09-20 15:43:58.117424: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2024-09-20 15:43:58.117501: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
            "2024-09-20 15:43:58.124308: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2024-09-20 15:43:58.124396: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
            "2024-09-20 15:43:58.126813: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2024-09-20 15:43:58.126890: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
            "2024-09-20 15:43:58.131914: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2024-09-20 15:43:58.131990: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
            "2024-09-20 15:43:58.138683: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2024-09-20 15:43:58.138750: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
            "2024-09-20 15:43:58.157105: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2024-09-20 15:43:58.157203: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round  0, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('accuracy', 0.47953695), ('loss', 7.4270635), ('num_examples', 69540), ('num_batches', 198)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('accuracy', 0.47808456), ('loss', 7.408291), ('num_examples', 69540), ('num_batches', 198)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  2, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('accuracy', 0.47745183), ('loss', 7.4082904), ('num_examples', 69540), ('num_batches', 198)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  3, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('accuracy', 0.47730803), ('loss', 7.394205), ('num_examples', 69540), ('num_batches', 198)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  4, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('accuracy', 0.4736123), ('loss', 7.415459), ('num_examples', 69540), ('num_batches', 198)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  5, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('accuracy', 0.4749928), ('loss', 7.403161), ('num_examples', 69540), ('num_batches', 198)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  6, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('accuracy', 0.47338223), ('loss', 7.3961315), ('num_examples', 69540), ('num_batches', 198)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  7, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('accuracy', 0.47241876), ('loss', 7.3823423), ('num_examples', 69540), ('num_batches', 198)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  8, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('accuracy', 0.4719442), ('loss', 7.3801365), ('num_examples', 69540), ('num_batches', 198)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  9, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('accuracy', 0.47175726), ('loss', 7.3621697), ('num_examples', 69540), ('num_batches', 198)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n"
          ]
        }
      ],
      "source": [
        "for round_num in range(NUM_ROUNDS):\n",
        "  result = training_process.next(train_state, federated_train_data)\n",
        "  train_state = result.state\n",
        "  train_metrics = result.metrics\n",
        "  print('round {:2d}, metrics={}'.format(round_num, train_metrics))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "e5fGtIJYNqYH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'tensorflow_federated.python.learning.templates.composers.LearningAlgorithmState'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-09-20 15:44:16.942725: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2024-09-20 15:44:16.942845: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tEval: loss=135.884, accuracy=0.485\n"
          ]
        }
      ],
      "source": [
        "def keras_evaluate(state):\n",
        "  # Take our global model weights and push them back into a Keras model to\n",
        "  # use its standard `.evaluate()` method.\n",
        "  keras_model = create_keras_model()\n",
        "  keras_model.compile(\n",
        "      loss=tf.keras.losses.BinaryFocalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.Accuracy()])\n",
        "  model_weights = training_process.get_model_weights(state)\n",
        "  model_weights.assign_weights_to(keras_model)\n",
        "  loss, accuracy = keras_model.evaluate(x=test_df.iloc[:,:-1], y=test_df.iloc[:,-1], steps=2, verbose=0)\n",
        "  print('\\tEval: loss={l:.3f}, accuracy={a:.3f}'.format(l=loss, a=accuracy))\n",
        "print(type(train_state))\n",
        "\n",
        "keras_evaluate(train_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "federated_learning_for_image_classification.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
