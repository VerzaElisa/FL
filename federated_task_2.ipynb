{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BKyHkMxKHfV"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_federated as tff\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "from keras.models import Sequential \n",
        "from keras.initializers import HeNormal, GlorotNormal\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.metrics import MeanSquaredError, CosineSimilarity, MeanAbsoluteError\n",
        "\n",
        "\n",
        "TEST_SIZE = 0.2\n",
        "NUM_CLIENTS = 10\n",
        "BATCH_SIZE = 512\n",
        "DROPOUT = 0.1\n",
        "EPOCHS = 10\n",
        "PREFETCH_BUFFER = 10\n",
        "NUM_ROUNDS = 10\n",
        "UNBALANCED = True\n",
        "path = os.path.dirname(tff.__file__)\n",
        "print(path)\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NayDhCX6SjwE"
      },
      "outputs": [],
      "source": [
        "# Import del dataset e divisione in train e test\n",
        "train_df = pd.read_csv('datasets/train_BMI.csv')\n",
        "test_df = pd.read_csv('datasets/test_BMI.csv')\n",
        "\n",
        "train_x = train_df.drop(columns=['label'])\n",
        "train_y = train_df['label'].astype('float32').values.reshape(-1, 1)\n",
        "\n",
        "test_x = test_df.drop(columns=['label'])\n",
        "test_y = test_df['label'].astype('float32').values.reshape(-1, 1)\n",
        "\n",
        "# Funzione per il preprocessing dei dati del singolo client che divide il dataset in batch\n",
        "def preprocess(dataset):\n",
        "  return dataset.repeat(EPOCHS).batch(BATCH_SIZE).prefetch(PREFETCH_BUFFER)\n",
        "\n",
        "# Funzione per aggiungere una colonna client_num al dataset in modo tale che ogni client abbia una percentuale di \n",
        "# righe del dataset diversa.\n",
        "def client_unbalanced(dataset, num_clients):\n",
        "    client_num = []\n",
        "    prob = np.random.pareto(1, num_clients)\n",
        "    prob /= np.sum(prob)\n",
        "    print(prob)\n",
        "    for i in range(len(dataset)):\n",
        "        client_num.append(np.random.choice(num_clients, p=prob))\n",
        "    print(len(client_num))\n",
        "\n",
        "    print([client_num.count(x) for x in range(num_clients)])\n",
        "    dataset['client_num'] = client_num\n",
        "    return dataset\n",
        "\n",
        "# Funzione per la creazione di un dataset ClientData a partire dal dataset di training a cui viene\n",
        "# aggiunta una colonna client_num che assegna ad ogni riga un client randomico\n",
        "def create_clients(dataset, unbalanced, num_clients=NUM_CLIENTS):\n",
        "    if unbalanced: \n",
        "        dataset = client_unbalanced(dataset, num_clients)\n",
        "    else:\n",
        "        # Viene creata una lista randomica di client\n",
        "        client_nums = list(range(num_clients))\n",
        "        generator = np.random.default_rng(42)\n",
        "        clients = generator.choice(client_nums, len(dataset))\n",
        "        dataset['client_num'] = clients\n",
        "\n",
        "    # Viene convertito il dataset in dizionari, uno per ogni client, con label e pixel associati\n",
        "    client_train_dataset = collections.OrderedDict()\n",
        "    grouped_dataset = dataset.groupby('client_num')\n",
        "    for key, item in grouped_dataset:\n",
        "        current_client = grouped_dataset.get_group(key)\n",
        "        data = collections.OrderedDict((('y', train_y), ('x', train_x)))\n",
        "        client_train_dataset[key] = data\n",
        "\n",
        "    # I dizionari vengono convertiti in ClientDataset\n",
        "    def serializable_dataset_fn(client_id):\n",
        "        client_data = client_train_dataset[client_id]\n",
        "        return tf.data.Dataset.from_tensor_slices(client_data)\n",
        "\n",
        "    tff_train_data = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
        "        client_ids=list(client_train_dataset.keys()),\n",
        "        serializable_dataset_fn=serializable_dataset_fn\n",
        "    )\n",
        "\n",
        "    return tff_train_data\n",
        "\n",
        "# Creazione della lista contenente i client con i relativi dataset\n",
        "elem_spec = {}\n",
        "def init(dataset, active_clients=NUM_CLIENTS, unbalanced=False): \n",
        "    client_data_df = create_clients(dataset, unbalanced, active_clients)\n",
        "    client_ids = sorted(client_data_df.client_ids)[:active_clients]\n",
        "    return [preprocess(client_data_df.create_tf_dataset_for_client(x)) for x in client_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LYCsJGJFWbqt"
      },
      "outputs": [],
      "source": [
        "def create_keras_model():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Dense(128, kernel_initializer=HeNormal(seed=42),input_dim = train_x.shape[1], activation='relu', kernel_regularizer = tf.keras.regularizers.l2(30e-6)))\n",
        "  model.add(Dropout(DROPOUT))\n",
        "  model.add(Dense(256, kernel_initializer=HeNormal(seed=42),activation='relu', kernel_regularizer = tf.keras.regularizers.l2(30e-6)))\n",
        "  model.add(Dropout(DROPOUT))\n",
        "  model.add(Dense(128, kernel_initializer=HeNormal(seed=42),activation='relu', kernel_regularizer = tf.keras.regularizers.l2(30e-6)))\n",
        "  model.add(Dense(64, kernel_initializer=HeNormal(seed=42),activation='relu', kernel_regularizer = tf.keras.regularizers.l2(30e-6)))\n",
        "  model.add(Dense(1, kernel_initializer=GlorotNormal(seed=42),activation='linear'))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Q3ynrxd53HzY"
      },
      "outputs": [],
      "source": [
        "def model_fn():\n",
        "  keras_model = create_keras_model()\n",
        "  return tff.learning.models.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=elem_spec,\n",
        "      loss=tf.keras.losses.MeanSquaredError(),\n",
        "      metrics=[MeanSquaredError(), MeanAbsoluteError(), CosineSimilarity()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def aggregator(algo, prox):\n",
        "    initial_learning_rate = 0.01\n",
        "    final_learning_rate = 0.001\n",
        "    learning_rate_decay_factor = (final_learning_rate / initial_learning_rate)**(1/10)\n",
        "    steps_per_epoch = int(train_x.shape[0]/BATCH_SIZE)\n",
        "\n",
        "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate,\n",
        "        decay_steps=steps_per_epoch,\n",
        "        decay_rate=learning_rate_decay_factor,\n",
        "        staircase=True)\n",
        "\n",
        "    def client_optimizer_fn(lr):\n",
        "        return tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "    if algo == 'weighted avg':\n",
        "        training_process = tff.learning.algorithms.build_weighted_fed_avg_with_optimizer_schedule(model_fn, \n",
        "                                                                                                  client_learning_rate_fn=lr_schedule,   \n",
        "                                                                                                  client_optimizer_fn=client_optimizer_fn)\n",
        "    if algo == 'unweighted avg':\n",
        "        training_process = tff.learning.algorithms.build_unweighted_fed_avg(model_fn, \n",
        "                                                                            client_optimizer_fn=tff.learning.optimizers.build_adam(learning_rate=0.001),\n",
        "                                                                            server_optimizer_fn=tff.learning.optimizers.build_adam(learning_rate=0.001))\n",
        "    if algo == 'weighted prox':\n",
        "        training_process = tff.learning.algorithms.build_weighted_fed_prox(model_fn, \n",
        "                                                                           proximal_strength=prox, \n",
        "                                                                           client_optimizer_fn=tff.learning.optimizers.build_adam(learning_rate=0.001),\n",
        "                                                                           server_optimizer_fn=tff.learning.optimizers.build_adam(learning_rate=0.001))\n",
        "    if algo == 'unweighted prox':\n",
        "        training_process = tff.learning.algorithms.build_weighted_fed_prox(model_fn, \n",
        "                                                                           proximal_strength=prox,\n",
        "                                                                           client_optimizer_fn=tff.learning.optimizers.build_adam(learning_rate=0.001),\n",
        "                                                                           server_optimizer_fn=tff.learning.optimizers.build_adam(learning_rate=0.001))\n",
        "    return training_process\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrJkQuCRJP9C"
      },
      "outputs": [],
      "source": [
        "federated_train_data = init(train_df)\n",
        "elem_spec = federated_train_data[0].element_spec\n",
        "training_process = aggregator('weighted prox', 20.0)\n",
        "train_state = training_process.initialize()\n",
        "for round_num in range(NUM_ROUNDS):\n",
        "  result = training_process.next(train_state, federated_train_data)\n",
        "  train_state = result.state\n",
        "  train_metrics = result.metrics\n",
        "  print('round {:2d}, metrics={}'.format(round_num, train_metrics))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(NUM_ROUNDS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "e5fGtIJYNqYH"
      },
      "outputs": [],
      "source": [
        "def keras_evaluate(state, training_process):\n",
        "  keras_model = create_keras_model()\n",
        "  keras_model.compile(\n",
        "      loss=MeanSquaredError(),\n",
        "      metrics=[MeanSquaredError(), MeanAbsoluteError(), CosineSimilarity()])\n",
        "  model_weights = training_process.get_model_weights(state)\n",
        "  model_weights.assign_weights_to(keras_model)\n",
        "  mse = keras_model.evaluate(x=test_x, y=test_y)\n",
        "  print('\\tEval: mse={a:.3f}, mae={b:.3f}, cs={c:.3f}'.format(a=mse[1], b=mse[2], c=mse[3]))\n",
        "  return mse[1], mse[2], mse[3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "keras_evaluate(train_state, training_process)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esperimenti\n",
        "==============\n",
        "\n",
        "***Algoritmo di aggregazione***\\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "federated_train_data = init(train_df)\n",
        "elem_spec = federated_train_data[0].element_spec\n",
        "# Tuning del parametro di proximal strength\n",
        "def tune_proximal_strength():\n",
        "    prox_list = []\n",
        "    for i in [1.0, 10.0, 20.0, 128.0, 256.0, 512.0]:\n",
        "        training_process = aggregator('weighted prox', i)\n",
        "        train_state = training_process.initialize()\n",
        "        curr = []\n",
        "        for round_num in range(NUM_ROUNDS):\n",
        "            result = training_process.next(train_state, federated_train_data)\n",
        "            train_state = result.state\n",
        "            train_metrics = result.metrics\n",
        "            print('round {:2d}, metrics={}'.format(round_num, train_metrics))\n",
        "            acc_tuple = (round_num, \n",
        "                         train_metrics['client_work']['train']['mean_squared_error'], \n",
        "                         train_metrics['client_work']['train']['mean_absolute_error'], \n",
        "                         train_metrics['client_work']['train']['cosine_similarity'])\n",
        "            curr.append(acc_tuple)\n",
        "        prox_list.append((i, curr))\n",
        "\n",
        "    return prox_list\n",
        "\n",
        "prox_list = tune_proximal_strength()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
        "i = 0\n",
        "rounds = []\n",
        "mse = []\n",
        "mae = []\n",
        "cs = []\n",
        "for algo, acc_list in prox_list:\n",
        "    rounds.append([x[0] for x in acc_list])\n",
        "    mse.append([x[1] for x in acc_list])\n",
        "    mae.append([x[2] for x in acc_list])\n",
        "    cs.append([x[3] for x in acc_list])\n",
        "    i+=1\n",
        "\n",
        "def plot_metrics(ax, rounds, metrics, labels=[1, 10, 20, 128, 256, 512]):\n",
        "    for i in range(len(metrics)):\n",
        "        ax.plot(rounds, metrics[i], label=labels[i])\n",
        "        ax.legend(loc='lower right')\n",
        "\n",
        "print(mse[0])\n",
        "plot_metrics(axs[0, 0], rounds[0], mse)\n",
        "plot_metrics(axs[0, 1], rounds[1], mae)\n",
        "plot_metrics(axs[1, 0], rounds[2], cs)\n",
        "\n",
        "axs[0, 0].set_title('mse')\n",
        "axs[0, 1].set_title('mae')\n",
        "axs[1, 0].set_title('cs')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "federated_train_data = init(train_df, unbalanced=True)\n",
        "elem_spec = federated_train_data[0].element_spec\n",
        "\n",
        "def agg_experiment():\n",
        "    prox_list = []\n",
        "    for i in ['weighted avg', 'unweighted avg', 'weighted prox', 'unweighted prox']:\n",
        "        training_process = aggregator(i, 1.0)\n",
        "        train_state = training_process.initialize()\n",
        "        curr = []\n",
        "        for round_num in range(NUM_ROUNDS):\n",
        "            result = training_process.next(train_state, federated_train_data)\n",
        "            train_state = result.state\n",
        "            train_metrics = result.metrics\n",
        "            print('round {:2d}, metrics={}'.format(round_num, train_metrics))\n",
        "            acc_tuple = (round_num, \n",
        "                         train_metrics['client_work']['train']['mean_squared_error'], \n",
        "                         train_metrics['client_work']['train']['mean_absolute_error'], \n",
        "                         train_metrics['client_work']['train']['cosine_similarity'])\n",
        "            curr.append(acc_tuple)\n",
        "        prox_list.append((i, curr))\n",
        "    return prox_list\n",
        "\n",
        "agg_algo_list = agg_experiment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
        "i = 0\n",
        "rounds = []\n",
        "mse = []\n",
        "mae = []\n",
        "cs = []\n",
        "for algo, acc_list in agg_algo_list:\n",
        "    rounds.append([x[0] for x in acc_list])\n",
        "    mse.append([x[1] for x in acc_list])\n",
        "    mae.append([x[2] for x in acc_list])\n",
        "    cs.append([x[3] for x in acc_list])\n",
        "    i+=1\n",
        "\n",
        "def plot_metrics(ax, rounds, metrics, labels=['weighted avg', 'unweighted avg', 'weighted prox', 'unweighted prox']):\n",
        "    for i in range(len(metrics)):\n",
        "        ax.plot(rounds, metrics[i], label=labels[i])\n",
        "        ax.legend(loc='upper right')\n",
        "\n",
        "print(mse[0])\n",
        "plot_metrics(axs[0, 0], rounds[0], mse)\n",
        "plot_metrics(axs[0, 1], rounds[1], mae)\n",
        "plot_metrics(axs[1, 0], rounds[2], cs)\n",
        "\n",
        "axs[0, 0].set_title('mse')\n",
        "axs[0, 1].set_title('mae')\n",
        "axs[1, 0].set_title('cs')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Numero e Percentuale clients***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "def client_perc_experiment():\n",
        "    client_list = []\n",
        "    eval_list = []\n",
        "    for i in [10, 50, 100, 500]:\n",
        "        perc_list = []\n",
        "        for j in [0.25, 0.50, 0.75, 1]:\n",
        "            federated_train_data = init(train_df, active_clients=math.floor(i*j))\n",
        "            global elem_spec \n",
        "            elem_spec = federated_train_data[0].element_spec\n",
        "            training_process = aggregator('weighted avg', 1.0)\n",
        "            train_state = training_process.initialize()\n",
        "  \n",
        "            curr = []\n",
        "            for round_num in range(NUM_ROUNDS):\n",
        "                result = training_process.next(train_state, federated_train_data)\n",
        "                train_state = result.state\n",
        "                train_metrics = result.metrics\n",
        "                print('round {:2d}, metrics={}'.format(round_num, train_metrics))\n",
        "                acc_tuple = (round_num, \n",
        "                         train_metrics['client_work']['train']['mean_squared_error'], \n",
        "                         train_metrics['client_work']['train']['mean_absolute_error'], \n",
        "                         train_metrics['client_work']['train']['cosine_similarity'])\n",
        "                curr.append(acc_tuple)\n",
        "            eval = keras_evaluate(train_state, training_process)\n",
        "            eval_list.append((i, j, eval))\n",
        "            perc_list.append((j, curr))\n",
        "        client_list.append((i, perc_list))\n",
        "    return client_list, eval_list\n",
        "\n",
        "client_list, eval_list = client_perc_experiment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(client_list, eval_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_metric(data, metric_index, metric_name):\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    for i, (clients, portions_data) in enumerate(data):\n",
        "        ax = axs[i // 2, i % 2]\n",
        "        for portion, epoch_data in portions_data:\n",
        "            epochs = [e[0] for e in epoch_data]\n",
        "            metric_values = [e[metric_index] for e in epoch_data]\n",
        "            ax.plot(epochs, metric_values, label=f'{portion*100}% data')\n",
        "        \n",
        "        ax.set_title(f'{clients} Clients')\n",
        "        ax.set_xlabel('Metrics')\n",
        "        ax.set_ylabel(metric_name)\n",
        "        ax.legend()\n",
        "        ax.grid(True) \n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "plot_metric(client_list, 1, 'MSE')\n",
        "plot_metric(client_list, 2, 'MAE')\n",
        "plot_metric(client_list, 3, 'CS') "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "federated_learning_for_image_classification.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
