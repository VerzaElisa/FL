{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "/home/ella/Documents/FL/venv-federated/lib/python3.9/site-packages/tensorflow_federated\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "\n",
    "import tensorflow_federated as tff\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys \n",
    "import matplotlib \n",
    "import scipy as sp \n",
    "import IPython\n",
    "from IPython import display \n",
    "import sklearn \n",
    "import random\n",
    "import time\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import f_oneway\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('-'*25)\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "NUM_CLIENTS = 5\n",
    "ACTIVE_CLIENTS = 5\n",
    "BATCH_SIZE = 5\n",
    "path = os.path.dirname(tff.__file__)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs disponibili:  []\n",
      "TensorFlow non sta usando la GPU\n"
     ]
    }
   ],
   "source": [
    "# Lista delle GPU disponibili\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs disponibili: \", gpus)\n",
    "\n",
    "# Verifica se TensorFlow utilizza la GPU\n",
    "if gpus:\n",
    "    print(\"TensorFlow sta usando la GPU\")\n",
    "else:\n",
    "    print(\"TensorFlow non sta usando la GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import del dataset e divisione in train e test\n",
    "train_df = pd.read_csv('datasets/train.csv')\n",
    "test_df = pd.read_csv('datasets/test.csv')\n",
    "\n",
    "# Viene diviso il train set in train e validation set\n",
    "train_df, val_df = train_test_split(train_df, test_size = TEST_SIZE, random_state = 42)\n",
    "\n",
    "train_x = train_df.iloc[:,:-1]\n",
    "train_y = train_df.iloc[:,-1]\n",
    "\n",
    "val_x = val_df.iloc[:,:-1]\n",
    "val_y = val_df.iloc[:,-1]\n",
    "\n",
    "test_x = test_df.iloc[:,:-1]\n",
    "test_y = test_df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per calcolare la dipendenza tra feature categoriche con il chi-square test.\n",
    "# Il test ha come ipotesi nulla che le due variabili siano indipendenti. Quindi con un p-value\n",
    "# minore di 0.05 si rigetta l'ipotesi nulla e si accetta che le due variabili sono dipendenti.\n",
    "def chi_square(f1, f2):\n",
    "    crosstab_result=pd.crosstab(index=f1,columns=f2)\n",
    "    return chi2_contingency(crosstab_result)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategia: Home_planet\n",
    "# Si nota la forte correlazione tra group ed home planet/destination e quindi per i nan si possono assegnre i valori del gruppo\n",
    "# (che saranno uguali alla moda). Restano problematici i gruppi composti da una sola persona, si introduce quindi una nuova categoria\n",
    "# di home planet e destination per persona con origine o destinazione sconosciuta.\n",
    "\n",
    "def mv_hp_d(dataset):\n",
    "    print(\"-------------------------- HomePlanet -------------------------\")\n",
    "\n",
    "    # HomePlanet\n",
    "    p = chi_square(dataset['HomePlanet'], dataset['Group'])\n",
    "    print(f\"Le feature HomePlanet e Group sono correlate, con p-value{p}\" if p < 0.05 \\\n",
    "                                                                           else \"Le feature Destination e Group non sono correlate\")\n",
    "\n",
    "    print(f\"Numero di persone con HomePlanet nullo: {str(dataset['HomePlanet'].isna().sum())}\")\n",
    "    dataset['HomePlanet'][(dataset['Solo']==0)] = dataset[(dataset['Solo']==0)].groupby('Group')['HomePlanet'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "    dataset['HomePlanet'][(dataset['Solo']==1)] = dataset[(dataset['Solo']==1)].groupby('Group')['HomePlanet'].transform(lambda x: x.fillna(\"unknown\"))\n",
    "    print(f\"Numero di persone con HomePlanet nullo: {str(dataset['HomePlanet'].isna().sum())}\")\n",
    "\n",
    "    # Destination\n",
    "    print(\"-------------------------- Destination ------------------------\")\n",
    "    p = chi_square(dataset['Destination'], dataset['Group'])\n",
    "    print(f\"Le feature Destination e Group sono correlate, con p-value{p}\" if p < 0.05 \\\n",
    "                                                                           else \"Le feature Destination e Group non sono correlate\")\n",
    "    \n",
    "    print(f\"Numero di persone con Destination nullo: {str(dataset['Destination'].isna().sum())}\")\n",
    "    dataset['Destination'][(dataset['Solo']==0)] = dataset[(dataset['Solo']==0)].groupby('Group')['Destination'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "    dataset['Destination'][(dataset['Solo']==1)] = dataset[(dataset['Solo']==1)].groupby('Group')['Destination'].transform(lambda x: x.fillna(\"unknown\"))\n",
    "    print(f\"Numero di persone con Destination nullo: {str(dataset['Destination'].isna().sum())}\")\n",
    "\n",
    "# Strategia: Age\n",
    "# Si distinguono 3 casi di persone con Age nullo: persone sole, persone con famiglia che non spendono e persone con famiglia che spendono.\n",
    "# Per assegnare l'età si usa la mediana del caso di appartenenza, immaginando che è meno probabile che una persona sola sia un bambino\n",
    "# e che al contrario è più probabile che una persona che non spende, ma è in un gruppo lo sia.\n",
    "def mv_age(dataset):\n",
    "    print(\"----------------------------- Age -----------------------------\")\n",
    "    solo = dataset['Solo']==1\n",
    "    family_no_spending = (dataset['Family_size']>1) & (dataset['No_spending']==1)\n",
    "    family_spending = (dataset['Family_size']>1) & (dataset['No_spending']==0)\n",
    "    \n",
    "    print(f\"Numero di persone con Age nullo: {str(dataset['Age'].isna().sum())}, di cui:\")\n",
    "    print(f\"Persone sole: {str(dataset['Age'][solo].isna().sum())}\")\n",
    "    print(f\"Persone con famiglia che non spendono: {str(dataset['Age'][family_no_spending].isna().sum())}\")   \n",
    "    print(f\"Persone con famiglia che spendono: {str(dataset['Age'][family_spending].isna().sum())}\")\n",
    "\n",
    "    dataset['Age'][solo] = dataset[solo]['Age'].transform(lambda x: x.fillna(dataset[solo]['Age'].median()))\n",
    "    dataset['Age'][family_no_spending] = dataset[family_no_spending]['Age'].transform(lambda x: x.fillna(dataset[family_no_spending]['Age'].median()))\n",
    "    dataset['Age'][family_spending] = dataset[family_spending]['Age'].transform(lambda x: x.fillna(dataset[family_spending]['Age'].median()))\n",
    "\n",
    "    print(f\"Numero di persone con Age nullo: {str(dataset['Age'].isna().sum())}\")   \n",
    "\n",
    "\n",
    "# Strategia: Surname\n",
    "# Si osserva che i gruppi sono per gran parte composti da persone con lo stesso cognome, quindi si può assegnare la moda del cognome del gruppo\n",
    "# alle persone con cognome nullo. Per i gruppi composti da una sola persona si assegna cognome 'unknown'.\n",
    "def mv_surname(dataset):\n",
    "    print(\"------------------------------ Surname ------------------------\")\n",
    "\n",
    "    p = chi_square(dataset['Surname'], dataset['Group'])\n",
    "    print(f\"Le feature Surname e Group sono correlate, con p-value{p}\" if p < 0.05 \\\n",
    "                                                                       else \"Le feature Surname e Group non sono correlate\")\n",
    "\n",
    "    print(f\"Numero di persone con Surname nullo: {str(dataset['Surname'].isna().sum())}\")\n",
    "    dataset['Surname'][(dataset['Solo']==0)] = dataset[(dataset['Solo']==0)].groupby('Group')['Surname'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "    dataset['Surname'][(dataset['Solo']==1)] = dataset[(dataset['Solo']==1)].groupby('Group')['Surname'].transform(lambda x: x.fillna(\"Unknown\"))\n",
    "    print(f\"Numero di persone con Surname nullo: {str(dataset['Surname'].isna().sum())}\")\n",
    "\n",
    "# Strategia: RoomService, FoodCourt, ShoppingMall, Spa, VRDeck\n",
    "# Si sostituiscono i valori nulli con la media di spesa delle altre categorie non nulle, dopo aver osservato che ogni persona ha almeno un valore\n",
    "# di spesa non nullo. Ad esempio, se una persona ha spesa nulla per RoomService e ShoppingMall, ma ha spesa per FoodCourt e Spa, si assegna la media\n",
    "# di FoodCourt e Spa per RoomService e ShoppingMall.\n",
    "def mv_exp(dataset):\n",
    "    print(\"------ RoomService, FoodCourt, ShoppingMall, Spa, VRDeck ------\")\n",
    "    print(f\"Persone con servizi nulli: {str(dataset[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].isnull().any(axis=1).sum())}\")\n",
    "    # Per ogni persona con almeno un servizio nullo, si calcola la media delle spese non nulle\n",
    "    dataset['RoomService'] = dataset['RoomService'].fillna(dataset[['FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].mean(axis=1))\n",
    "    dataset['FoodCourt'] = dataset['FoodCourt'].fillna(dataset[['RoomService', 'ShoppingMall', 'Spa', 'VRDeck']].mean(axis=1))\n",
    "    dataset['ShoppingMall'] = dataset['ShoppingMall'].fillna(dataset[['RoomService', 'FoodCourt', 'Spa', 'VRDeck']].mean(axis=1))\n",
    "    dataset['Spa'] = dataset['Spa'].fillna(dataset[['RoomService', 'FoodCourt', 'ShoppingMall', 'VRDeck']].mean(axis=1))\n",
    "    dataset['VRDeck'] = dataset['VRDeck'].fillna(dataset[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa']].mean(axis=1))\n",
    "    print(f\"Persone con servizi nulli: {str(dataset[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].isnull().any(axis=1).sum())}\")\n",
    "\n",
    "# Controllare se i gruppi hanno tutti lo stesso valore di cabin_side\n",
    "def mv_cabins(dataset):\n",
    "    print(\"------------------------- Cabin_side --------------------------\")\n",
    "    p = chi_square(dataset['Cabin_side'], dataset['Group'])\n",
    "    print(f\"Le feature Cabin_side e Group sono correlate, con p-value{p}\" if p < 0.05 \\\n",
    "                                                                          else \"Le feature Cabin_side e Group non sono correlate\")\n",
    "    print(f\"Numero di persone con Cabin_side nullo: {str(dataset['Cabin_side'].isna().sum())}\")\n",
    "    dataset['Cabin_side'][(dataset['Solo']==0)] = dataset[(dataset['Solo']==0)].groupby('Group')['Cabin_side'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "    dataset['Cabin_side'][(dataset['Solo']==1)] = dataset['Cabin_side'][(dataset['Solo']==1)].fillna('unknown')\n",
    "    print(f\"Numero di persone con Cabin_side nullo: {str(dataset['Cabin_side'].isna().sum())}\")\n",
    "\n",
    "    print(\"------------------------ Cabin_number -------------------------\")\n",
    "    p = chi_square(dataset['Cabin_number'], dataset['Group'])\n",
    "    print(f\"Le feature Cabin_number e Group sono correlate, con p-value{p}\" if p < 0.05 \\\n",
    "                                                                            else \"Le feature Cabin_number e Group non sono correlate\")\n",
    "    print(f\"Numero di persone con Cabin_number nullo: {str(dataset['Cabin_number'].isna().sum())}\")\n",
    "    dataset['Cabin_number'][(dataset['Solo']==0)] = dataset[(dataset['Solo']==0)].groupby('Group')['Cabin_number'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "    dataset['Cabin_number'][(dataset['Solo']==1)] = dataset['Cabin_number'][(dataset['Solo']==1)].fillna(0)\n",
    "    print(f\"Numero di persone con Cabin_number nullo: {str(dataset['Cabin_number'].isna().sum())}\")\n",
    "    \n",
    "    print(\"------------------------- Cabin_deck --------------------------\")\n",
    "    p = chi_square(dataset['Cabin_deck'], dataset['HomePlanet'])\n",
    "    print(f\"Le feature Cabin_deck e HomePlanet sono correlate, con p-value{p}\" if p < 0.05 \\\n",
    "                                                                               else \"Le feature Cabin_deck e HomePlanet non sono correlate\")\n",
    "    print(f\"Numero di persone con Cabin_deck nullo: {str(dataset['Cabin_deck'].isna().sum())}\")\n",
    "    dataset['Cabin_deck'][(dataset['Solo']==0)] = dataset[(dataset['Solo']==0)].groupby('HomePlanet')['Cabin_deck'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "    dataset['Cabin_deck'][(dataset['Solo']==1)] = dataset['Cabin_deck'][(dataset['Solo']==1)].fillna('unknown')\n",
    "    print(f\"Numero di persone con Cabin_deck nullo: {str(dataset['Cabin_deck'].isna().sum())}\")\n",
    "\n",
    "# Strategia: VIP, CrioSleep\n",
    "# Per le feature VIP e CrioSleep si assegna il valore della moda a tutti i valori nulli.\n",
    "def mv_vip_crio(dataset):\n",
    "    print(\"----------------------------- VIP, CryoSleep ------------------\")\n",
    "    print(f\"Numero di persone con VIP nullo: {str(dataset['VIP'].isna().sum())}\")\n",
    "    dataset['VIP'] = dataset['VIP'].fillna(dataset['VIP'].mode()[0])\n",
    "    print(f\"Numero di persone con VIP nullo: {str(dataset['VIP'].isna().sum())}\")\n",
    "\n",
    "    print(f\"Numero di persone con CryoSleep nullo: {str(dataset['CryoSleep'].isna().sum())}\")\n",
    "    dataset['CryoSleep'] = dataset['CryoSleep'].fillna(dataset['CryoSleep'].mode()[0])\n",
    "    print(f\"Numero di persone con CryoSleep nullo: {str(dataset['CryoSleep'].isna().sum())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age',\n",
      "       'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',\n",
      "       'Name'],\n",
      "      dtype='object')\n",
      "------ RoomService, FoodCourt, ShoppingMall, Spa, VRDeck ------\n",
      "Persone con servizi nulli: 691\n",
      "Persone con servizi nulli: 0\n",
      "-------------------------- HomePlanet -------------------------\n",
      "Le feature HomePlanet e Group sono correlate, con p-value7.383208949075836e-101\n",
      "Numero di persone con HomePlanet nullo: 168\n",
      "Numero di persone con HomePlanet nullo: 0\n",
      "-------------------------- Destination ------------------------\n",
      "Le feature Destination e Group sono correlate, con p-value0.00016495054761429579\n",
      "Numero di persone con Destination nullo: 139\n",
      "Numero di persone con Destination nullo: 0\n",
      "------------------------------ Surname ------------------------\n",
      "Le feature Surname e Group sono correlate, con p-value0.0\n",
      "Numero di persone con Surname nullo: 159\n",
      "Numero di persone con Surname nullo: 0\n",
      "----------------------------- Age -----------------------------\n",
      "Numero di persone con Age nullo: 148, di cui:\n",
      "Persone sole: 87\n",
      "Persone con famiglia che non spendono: 40\n",
      "Persone con famiglia che spendono: 21\n",
      "Numero di persone con Age nullo: 0\n",
      "------------------------- Cabin_side --------------------------\n",
      "Le feature Cabin_side e Group sono correlate, con p-value3.8625169822992095e-51\n",
      "Numero di persone con Cabin_side nullo: 158\n",
      "Numero di persone con Cabin_side nullo: 0\n",
      "------------------------ Cabin_number -------------------------\n",
      "Le feature Cabin_number e Group sono correlate, con p-value0.0\n",
      "Numero di persone con Cabin_number nullo: 158\n",
      "Numero di persone con Cabin_number nullo: 0\n",
      "------------------------- Cabin_deck --------------------------\n",
      "Le feature Cabin_deck e HomePlanet sono correlate, con p-value0.0\n",
      "Numero di persone con Cabin_deck nullo: 158\n",
      "Numero di persone con Cabin_deck nullo: 0\n",
      "----------------------------- VIP, CryoSleep ------------------\n",
      "Numero di persone con VIP nullo: 162\n",
      "Numero di persone con VIP nullo: 0\n",
      "Numero di persone con CryoSleep nullo: 177\n",
      "Numero di persone con CryoSleep nullo: 0\n",
      "Index(['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age',\n",
      "       'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',\n",
      "       'Name'],\n",
      "      dtype='object')\n",
      "------ RoomService, FoodCourt, ShoppingMall, Spa, VRDeck ------\n",
      "Persone con servizi nulli: 217\n",
      "Persone con servizi nulli: 0\n",
      "-------------------------- HomePlanet -------------------------\n",
      "Le feature HomePlanet e Group sono correlate, con p-value0.0003119608334944179\n",
      "Numero di persone con HomePlanet nullo: 33\n",
      "Numero di persone con HomePlanet nullo: 0\n",
      "-------------------------- Destination ------------------------\n",
      "Le feature Destination e Group non sono correlate\n",
      "Numero di persone con Destination nullo: 43\n",
      "Numero di persone con Destination nullo: 0\n",
      "------------------------------ Surname ------------------------\n",
      "Le feature Surname e Group sono correlate, con p-value0.0\n",
      "Numero di persone con Surname nullo: 41\n",
      "Numero di persone con Surname nullo: 0\n",
      "----------------------------- Age -----------------------------\n",
      "Numero di persone con Age nullo: 31, di cui:\n",
      "Persone sole: 26\n",
      "Persone con famiglia che non spendono: 3\n",
      "Persone con famiglia che spendono: 2\n",
      "Numero di persone con Age nullo: 0\n",
      "------------------------- Cabin_side --------------------------\n",
      "Le feature Cabin_side e Group sono correlate, con p-value0.007907915616871108\n",
      "Numero di persone con Cabin_side nullo: 41\n",
      "Numero di persone con Cabin_side nullo: 0\n",
      "------------------------ Cabin_number -------------------------\n",
      "Le feature Cabin_number e Group sono correlate, con p-value0.0\n",
      "Numero di persone con Cabin_number nullo: 41\n",
      "Numero di persone con Cabin_number nullo: 0\n",
      "------------------------- Cabin_deck --------------------------\n",
      "Le feature Cabin_deck e HomePlanet sono correlate, con p-value0.0\n",
      "Numero di persone con Cabin_deck nullo: 41\n",
      "Numero di persone con Cabin_deck nullo: 0\n",
      "----------------------------- VIP, CryoSleep ------------------\n",
      "Numero di persone con VIP nullo: 41\n",
      "Numero di persone con VIP nullo: 0\n",
      "Numero di persone con CryoSleep nullo: 40\n",
      "Numero di persone con CryoSleep nullo: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_data(dataset):\n",
    "    print(dataset.columns)\n",
    "    # Viene aggiunta una colonna per contare il numero di persone nel gruppo di appartenenza e se la persona è da sola\n",
    "    dataset['Group'] = dataset['PassengerId'].apply(lambda x: x.split('_')[0]).astype(int)\n",
    "    dataset['Group_size']=dataset['Group'].map(lambda x: dataset['Group'].value_counts()[x])\n",
    "    dataset['Solo']=(dataset['Group_size']==1).astype(int)\n",
    "\n",
    "    # Viene creata una colonna per il cognome e si rimuove la colonna Name\n",
    "    dataset['Name'].fillna('Unknown Unknown', inplace=True)\n",
    "    dataset['Surname']=dataset['Name'].str.split().str[-1]\n",
    "    dataset.loc[dataset['Surname']=='Unknown','Surname']=np.nan\n",
    "    dataset.drop('Name', axis=1, inplace=True)\n",
    "\n",
    "    # Vengono riempiti i valori nulli delle colonne RoomService, FoodCourt, ShoppingMall, Spa, VRDeck\n",
    "    mv_exp(dataset)\n",
    "    \n",
    "    # Vengono prese tutte le colonne che rappresentano le spese e si aggiunge una colonna per contare il\n",
    "    # totale speso dalla persona e una colonna booleana per indicare se la persona non ha speso nulla\n",
    "    exp_feats=['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "    dataset['Expenditure']=dataset[exp_feats].sum(axis=1)\n",
    "    dataset['No_spending']=(dataset['Expenditure']==0).astype(int)\n",
    "\n",
    "    # Vengono riempiti i valori nulli della colonna HomePlanet, Destination, Surmane\n",
    "    mv_hp_d(dataset)\n",
    "    mv_surname(dataset)\n",
    "\n",
    "    # Viene aggiunta una colonna per contare le persone che hanno lo stesso cognome\n",
    "    dataset['Family_size']=dataset[(dataset['Surname']!='Unknown')].groupby('Group')['Surname'].transform('count')\n",
    "    dataset.loc[dataset['Surname']=='Unknown','Family_size']=1\n",
    "    dataset.loc[dataset['Family_size']>100,'Family_size']=np.nan\n",
    "\n",
    "    # Vengono riempiti i valori nulli della colonna Age\n",
    "    mv_age(dataset)\n",
    "\n",
    "\n",
    "    # Viene aggiunta una colonna che assegna ogni passeggero al gruppo di età di appartenenza\n",
    "    dataset['Age_group']=0\n",
    "    dataset.loc[dataset['Age']<=12,'Age_group']='Age_0-12'\n",
    "    dataset.loc[(dataset['Age']>12) & (dataset['Age']<18),'Age_group']='Age_13-17'\n",
    "    dataset.loc[(dataset['Age']>=18) & (dataset['Age']<=25),'Age_group']='Age_18-25'\n",
    "    dataset.loc[(dataset['Age']>25) & (dataset['Age']<=30),'Age_group']='Age_26-30'\n",
    "    dataset.loc[(dataset['Age']>30) & (dataset['Age']<=50),'Age_group']='Age_31-50'\n",
    "    dataset.loc[dataset['Age']>50,'Age_group']='Age_51+'\n",
    "\n",
    "    # Separazione della colonna 'Cabin' nelle colonne deck, number e side\n",
    "    dataset['Cabin'].fillna('Z/9999/Z', inplace=True)\n",
    "\n",
    "    dataset['Cabin_deck'] = dataset['Cabin'].apply(lambda x: x.split('/')[0])\n",
    "    dataset['Cabin_number'] = dataset['Cabin'].apply(lambda x: x.split('/')[1]).astype(int)\n",
    "    dataset['Cabin_side'] = dataset['Cabin'].apply(lambda x: x.split('/')[2])\n",
    "\n",
    "    dataset.loc[dataset['Cabin_deck']=='Z', 'Cabin_deck']=np.nan\n",
    "    dataset.loc[dataset['Cabin_number']==9999, 'Cabin_number']=np.nan\n",
    "    dataset.loc[dataset['Cabin_side']=='Z', 'Cabin_side']=np.nan\n",
    "\n",
    "    dataset.drop('Cabin', axis=1, inplace=True)\n",
    "    mv_cabins(dataset)\n",
    "    mv_vip_crio(dataset)\n",
    "\n",
    "\n",
    "preprocess_data(train_x)\n",
    "preprocess_data(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Feature  Importance\n",
      "15   Expenditure    0.104200\n",
      "20  Cabin_number    0.071537\n",
      "9            Spa    0.071411\n",
      "0    PassengerId    0.070535\n",
      "11         Group    0.065416\n",
      "14       Surname    0.065113\n",
      "10        VRDeck    0.062469\n",
      "7      FoodCourt    0.061913\n",
      "4            Age    0.060146\n",
      "2      CryoSleep    0.060006\n",
      "8   ShoppingMall    0.050029\n",
      "16   No_spending    0.049126\n",
      "6    RoomService    0.048928\n",
      "19    Cabin_deck    0.037988\n",
      "1     HomePlanet    0.027629\n",
      "18     Age_group    0.022367\n",
      "21    Cabin_side    0.018697\n",
      "3    Destination    0.018517\n",
      "17   Family_size    0.013819\n",
      "12    Group_size    0.013797\n",
      "13          Solo    0.005202\n",
      "5            VIP    0.001156\n",
      "         Feature  Importance\n",
      "15   Expenditure    0.105121\n",
      "20  Cabin_number    0.075874\n",
      "11         Group    0.071325\n",
      "14       Surname    0.071261\n",
      "0    PassengerId    0.070880\n",
      "4            Age    0.067348\n",
      "2      CryoSleep    0.065030\n",
      "9            Spa    0.062779\n",
      "10        VRDeck    0.059472\n",
      "7      FoodCourt    0.053802\n",
      "6    RoomService    0.050893\n",
      "8   ShoppingMall    0.046495\n",
      "16   No_spending    0.041629\n",
      "19    Cabin_deck    0.040184\n",
      "1     HomePlanet    0.032759\n",
      "18     Age_group    0.026979\n",
      "3    Destination    0.020629\n",
      "21    Cabin_side    0.017105\n",
      "17   Family_size    0.007073\n",
      "12    Group_size    0.006588\n",
      "13          Solo    0.005518\n",
      "5            VIP    0.001255\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "def feature_sel(dataset, label):\n",
    "    # Train the Random Forest model\n",
    "    dataset = dataset.apply(lambda col: col.astype(str) if col.dtype == 'object' else col)\n",
    "\n",
    "    # Seleziona le colonne numeriche e categoriali\n",
    "    num_cols = dataset.select_dtypes(exclude='object').columns\n",
    "    cat_cols = dataset.select_dtypes(include='object').columns\n",
    "\n",
    "    # Applica lo scaling ai dati numerici\n",
    "    scaler = MinMaxScaler()\n",
    "    dataset[num_cols] = scaler.fit_transform(dataset[num_cols])\n",
    "\n",
    "    # Applica l'encoding ai dati categoriali\n",
    "    encoder = OrdinalEncoder()\n",
    "    dataset[cat_cols] = encoder.fit_transform(dataset[cat_cols])\n",
    "\n",
    "    # Train the Random Forest model\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(dataset, label)\n",
    "\n",
    "    # Extract feature importances\n",
    "    importances = rf.feature_importances_\n",
    "    feature_names = dataset.columns\n",
    "    feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "\n",
    "    # Rank features by importance\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    print(feature_importance_df)\n",
    "\n",
    "    # Select top N features (example selecting top 10 features)\n",
    "    top_features = feature_importance_df['Feature'][(feature_importance_df['Importance']>=0.05)].values\n",
    "    return dataset[top_features]\n",
    "\n",
    "train_x_sel = feature_sel(train_x, train_y)\n",
    "val_x_sel = feature_sel(val_x, val_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expenditure     float64\n",
      "Cabin_number    float64\n",
      "Spa             float64\n",
      "PassengerId     float64\n",
      "Group           float64\n",
      "Surname         float64\n",
      "VRDeck          float64\n",
      "FoodCourt       float64\n",
      "Age             float64\n",
      "CryoSleep       float64\n",
      "ShoppingMall    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_x_sel = pd.get_dummies(train_x_sel, columns=train_x_sel.select_dtypes(include=['object', 'bool']).columns.difference(['Surname', 'Cabin_number']))\n",
    "val_x_sel = pd.get_dummies(val_x_sel, columns=val_x_sel.select_dtypes(include=['object', 'bool']).columns.difference(['Surname', 'Cabin_number']))\n",
    "print(train_x_sel.dtypes) \n",
    "# Convertire in float tutte le colonne\n",
    "train_x_sel = train_x_sel.astype(float)\n",
    "train_y = train_y.astype(float)\n",
    "\n",
    "val_x_sel = val_x_sel.astype(float)\n",
    "val_y = val_y.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expenditure     float64\n",
      "Cabin_number    float64\n",
      "Spa             float64\n",
      "PassengerId     float64\n",
      "Group           float64\n",
      "Surname         float64\n",
      "VRDeck          float64\n",
      "FoodCourt       float64\n",
      "Age             float64\n",
      "CryoSleep       float64\n",
      "ShoppingMall    float64\n",
      "dtype: object\n",
      "      Expenditure  Cabin_number       Spa  PassengerId     Group  Surname  \\\n",
      "2333     0.022041      0.000000  0.035322       1847.0  0.270640   1178.0   \n",
      "2589     0.038005      0.303590  0.000000       2061.0  0.298771   1188.0   \n",
      "8302     0.000000      0.173706  0.000000       6648.0  0.954947   1321.0   \n",
      "8177     0.049095      0.950370  0.052552       6549.0  0.941367   1512.0   \n",
      "500      0.000000      0.009504  0.000000        391.0  0.057879   1326.0   \n",
      "\n",
      "      VRDeck  FoodCourt       Age  CryoSleep  ShoppingMall  \n",
      "2333     0.0   0.001984  0.354430        0.0      0.000000  \n",
      "2589     0.0   0.043105  0.215190        0.0      0.001320  \n",
      "8302     0.0   0.000000  0.354430        1.0      0.000000  \n",
      "8177     0.0   0.000072  0.253165        0.0      0.012302  \n",
      "500      0.0   0.000000  0.455696        1.0      0.000000  \n"
     ]
    }
   ],
   "source": [
    "print(train_x_sel.dtypes)\n",
    "print(train_x_sel.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_53 (Dense)            (None, 9)                 108       \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 9)                 90        \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 5)                 50        \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 254 (1016.00 Byte)\n",
      "Trainable params: 254 (1016.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "14/14 [==============================] - 1s 11ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 2/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 3/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 4/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 5/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 6/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 7/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 8/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 9/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 10/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 11/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 12/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 13/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 14/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 15/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 16/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 17/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 18/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 19/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 20/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 21/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 22/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 23/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 24/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 25/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 26/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 27/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 28/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 29/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 30/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 31/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 32/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 33/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 34/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 35/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 36/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 37/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 38/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 39/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 40/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 41/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 42/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 43/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 44/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 45/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 46/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 47/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 48/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 49/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 50/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 51/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 52/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 53/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 54/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 55/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 56/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 57/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 58/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 59/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 60/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 61/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 62/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 63/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 64/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 65/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 66/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 67/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 68/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 69/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 70/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 71/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 72/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 73/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 74/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 75/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 76/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 77/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 78/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 79/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 80/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 81/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 82/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 83/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 84/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 85/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 86/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 87/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 88/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 89/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 90/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 91/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 92/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 93/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 94/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 95/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 96/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 97/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 98/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 99/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n",
      "Epoch 100/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5049\n"
     ]
    }
   ],
   "source": [
    "# Initialising the NN\n",
    "model = Sequential()\n",
    "\n",
    "# layers\n",
    "model.add(Dense(9, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "model.add(Dense(9, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(5, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# summary\n",
    "model.summary()\n",
    "# Compiling the NN\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Train the NN\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, min_delta=0.001, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_x_sel, train_y,\n",
    "                    epochs=100,\n",
    "                    batch_size=512,\n",
    "                    #callbacks=[early_stopping_cb],\n",
    "                    validation_data=(val_x_sel, val_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
