{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('-'*25)\n",
    "\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import del dataset e divisione in train e test\n",
    "train_df = pd.read_csv('datasets/InternetAddiction.csv')\n",
    "train_df.rename(columns={'sii':'label'}, inplace=True)\n",
    "train_df = train_df.dropna(subset=['label'])\n",
    "\n",
    "# Viene diviso il train set in train e validation set\n",
    "train, test_df = train_test_split(train_df, test_size = TEST_SIZE, random_state = 42)\n",
    "train_df, val_df = train_test_split(train, test_size = TEST_SIZE, random_state = 42)\n",
    "\n",
    "train_x = train_df.drop(columns=['label'])\n",
    "train_y = train_df['label'].astype(int)\n",
    "\n",
    "val_x = val_df.drop(columns=['label'])\n",
    "val_y = val_df['label'].astype(int)\n",
    "\n",
    "test_x = test_df.drop(columns=['label'])\n",
    "test_y = test_df['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_values(dataset):\n",
    "    dataset = dataset.drop(columns=['id'])\n",
    "    for col in dataset.columns:\n",
    "        if dataset[col].isnull().sum() > 0:\n",
    "            if (dataset[col].dtype == 'object') | (dataset[col].dtype == 'bool'):\n",
    "                dataset[col].fillna(dataset[col].mode()[0], inplace=True)\n",
    "            else:\n",
    "                dataset[col].fillna(dataset[col].mean(), inplace=True)\n",
    "    return dataset\n",
    "\n",
    "def fs(dataset, dataset_y):\n",
    "    str_path = 'objects/features_internet.npy'\n",
    "\n",
    "    if not os.path.exists(str_path):\n",
    "        dataset['label'] = dataset_y\n",
    "        correlation_matrix=dataset.corr()\n",
    "        features = correlation_matrix['label'][(correlation_matrix['label']>=0.1) | (correlation_matrix['label']<=-0.1)].index\n",
    "        features = features.drop('label')\n",
    "        print(correlation_matrix['label'].sort_values(ascending=False))\n",
    "        print(len(features))\n",
    "        np.save(str_path, features)\n",
    "    features = np.load(str_path, allow_pickle=True)\n",
    "    return dataset[features]\n",
    "\n",
    "def encode(dataset, dataset_y):\n",
    "    dataset = nan_values(dataset)\n",
    "    fill_dict = {col: dataset[col].mean() if dataset[col].dtype != 'object' else dataset[col].mode()[0] for col in dataset.columns}\n",
    "    cat_cols = dataset.select_dtypes(include='object').columns\n",
    "    num_cols = dataset.select_dtypes(exclude='object').columns\n",
    "    print(dataset.dtypes)\n",
    "    scaler = StandardScaler()\n",
    "    dataset[num_cols] = scaler.fit_transform(dataset[num_cols])\n",
    "\n",
    "    encoder = OrdinalEncoder()\n",
    "    dataset[cat_cols] = encoder.fit_transform(dataset[cat_cols])\n",
    "\n",
    "    dataset = fs(dataset, dataset_y)\n",
    "\n",
    "    return dataset.astype(float), scaler, encoder, fill_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_test_val(dataset, scaler, encoder, mean):\n",
    "    dataset = dataset.drop(columns=['id'])\n",
    "    for key in mean.keys():\n",
    "        dataset[key].fillna(mean[key], inplace=True)\n",
    "\n",
    "    cat_cols = dataset.select_dtypes(include='object').columns\n",
    "    num_cols = dataset.select_dtypes(exclude='object').columns\n",
    "    dataset[num_cols] = scaler.transform(dataset[num_cols])\n",
    "    dataset[cat_cols] = encoder.transform(dataset[cat_cols])\n",
    "\n",
    "    features = np.load('objects/features_internet.npy', allow_pickle=True)\n",
    "    dataset = dataset[features]\n",
    "    dataset = dataset.astype(float)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, scaler, encoder, mean = encode(train_x, train_y)\n",
    "val_x = preproc_test_val(val_x, scaler, encoder, mean)\n",
    "test_x = preproc_test_val(test_x, scaler, encoder, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 512\n",
    "DROPOUT = 0.2\n",
    "classes = 4\n",
    "\n",
    "train_y = to_categorical(train_y)\n",
    "val_y = to_categorical(val_y)\n",
    "test_y = to_categorical(test_y)\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "def model_fn():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(112, activation='softmax', input_dim=train_x.shape[-1]))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(272, activation='tanh'))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(classes,  kernel_initializer='normal', activation='softmax'))   \n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = model_fn()\n",
    "\n",
    "initial_learning_rate = 0.01\n",
    "final_learning_rate = 0.0001\n",
    "learning_rate_decay_factor = (final_learning_rate / initial_learning_rate)**(1/100)\n",
    "steps_per_epoch = int(train_x.shape[0]/BATCH_SIZE)\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=steps_per_epoch,\n",
    "    decay_rate=learning_rate_decay_factor,\n",
    "    staircase=True)\n",
    "model.compile(optimizer = keras.optimizers.Adam(learning_rate=lr_schedule), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20, min_delta=0.0001, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_x, train_y,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=[early_stopping_cb],\n",
    "                    validation_data=(val_x, val_y))\n",
    "\n",
    "EPOCHS = len(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].plot(history.history['val_accuracy'])\n",
    "axs[0].plot(history.history['accuracy'])\n",
    "axs[0].legend(['val_accuracy', 'accuracy'])\n",
    "\n",
    "axs[1].plot(history.history['val_loss'])\n",
    "axs[1].plot(history.history['loss'])\n",
    "axs[1].legend(['val_loss', 'loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train.drop(columns=['label'])\n",
    "train_y = train['label']\n",
    "\n",
    "test_x = test_df.drop(columns=['label'])\n",
    "test_y = test_df['label']\n",
    "\n",
    "train_y = to_categorical(train_y)\n",
    "test_y = to_categorical(test_y)\n",
    "\n",
    "train_x, scaler, encoder, mean = encode(train_x, train_y)\n",
    "test_x = preproc_test_val(test_x, scaler, encoder, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_fn() \n",
    "model.compile(optimizer = keras.optimizers.Adam(learning_rate=lr_schedule), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "history = model.fit(train_x, train_y,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE)\n",
    "\n",
    "score = model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_x, train['label']], axis=1)\n",
    "train_df.to_csv('datasets/train_internet.csv', index=False)\n",
    "test_df = pd.concat([test_x, test_df['label']], axis=1)\n",
    "test_df.to_csv('datasets/test_internet.csv', index=False)\n",
    "\n",
    "model.save('models/internet_model')\n",
    "model.save_weights('models/internet_weights')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-federated",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
